{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ygame (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -axlib (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ygame (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -axlib (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting muax==0.0.2.7.1a0\n",
      "  Downloading muax-0.0.2.7.1a0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: mctx in /opt/conda/lib/python3.7/site-packages (from muax==0.0.2.7.1a0) (0.0.2)\n",
      "Requirement already satisfied: lz4 in /opt/conda/lib/python3.7/site-packages (from muax==0.0.2.7.1a0) (4.0.2)\n",
      "Requirement already satisfied: optax in /opt/conda/lib/python3.7/site-packages (from muax==0.0.2.7.1a0) (0.1.4)\n",
      "Requirement already satisfied: dm-haiku in /opt/conda/lib/python3.7/site-packages (from muax==0.0.2.7.1a0) (0.0.9)\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/lib/python3.7/site-packages (from muax==0.0.2.7.1a0) (2.5.1)\n",
      "Requirement already satisfied: gymnasium in /opt/conda/lib/python3.7/site-packages (from muax==0.0.2.7.1a0) (0.27.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from dm-haiku->muax==0.0.2.7.1a0) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from dm-haiku->muax==0.0.2.7.1a0) (1.21.6)\n",
      "Requirement already satisfied: jmp>=0.0.2 in /opt/conda/lib/python3.7/site-packages (from dm-haiku->muax==0.0.2.7.1a0) (0.0.4)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from dm-haiku->muax==0.0.2.7.1a0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from dm-haiku->muax==0.0.2.7.1a0) (4.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/conda/lib/python3.7/site-packages (from gymnasium->muax==0.0.2.7.1a0) (5.1.0)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from gymnasium->muax==0.0.2.7.1a0) (0.2.0)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from gymnasium->muax==0.0.2.7.1a0) (0.0.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gymnasium->muax==0.0.2.7.1a0) (2.2.0)\n",
      "Requirement already satisfied: chex>=0.0.8 in /opt/conda/lib/python3.7/site-packages (from mctx->muax==0.0.2.7.1a0) (0.1.5)\n",
      "Requirement already satisfied: jax>=0.1.55 in /opt/conda/lib/python3.7/site-packages (from mctx->muax==0.0.2.7.1a0) (0.3.25)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /opt/conda/lib/python3.7/site-packages (from mctx->muax==0.0.2.7.1a0) (0.3.25+cuda11.cudnn82)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorboardX->muax==0.0.2.7.1a0) (3.19.6)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from chex>=0.0.8->mctx->muax==0.0.2.7.1a0) (0.12.0)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from chex>=0.0.8->mctx->muax==0.0.2.7.1a0) (0.1.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gymnasium->muax==0.0.2.7.1a0) (3.11.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/conda/lib/python3.7/site-packages (from jax>=0.1.55->mctx->muax==0.0.2.7.1a0) (1.7.3)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.7/site-packages (from jax>=0.1.55->mctx->muax==0.0.2.7.1a0) (3.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ygame (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -axlib (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: muax\n",
      "  Attempting uninstall: muax\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ygame (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -axlib (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: muax 0.0.2.7.1\n",
      "    Uninstalling muax-0.0.2.7.1:\n",
      "      Successfully uninstalled muax-0.0.2.7.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ygame (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -axlib (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed muax-0.0.2.7.1a0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ygame (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -axlib (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ygame (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -axlib (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ygame (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -axlib (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install muax==0.0.2.7.1a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "import gymnasium as gym "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use `muax.fit` to fit CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import muax\n",
    "from muax import nn "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`muax` provides example `representation`, `prediction` and `dynamic` modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_size = 10 \n",
    "embedding_size = 8\n",
    "discount = 0.99\n",
    "num_actions = 2\n",
    "full_support_size = int(support_size * 2 + 1)\n",
    "\n",
    "repr_fn = nn._init_representation_func(nn.Representation, embedding_size)\n",
    "pred_fn = nn._init_prediction_func(nn.Prediction, num_actions, full_support_size)\n",
    "dy_fn = nn._init_dynamic_func(nn.Dynamic, embedding_size, num_actions, full_support_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use your customized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp \n",
    "import haiku as hk\n",
    "\n",
    "\n",
    "class Representation(hk.Module):\n",
    "  def __init__(self, embedding_dim, name='representation'):\n",
    "    super().__init__(name=name)\n",
    "\n",
    "    self.repr_func = hk.Sequential([\n",
    "        hk.Linear(embedding_dim)\n",
    "    ])\n",
    "\n",
    "  def __call__(self, obs):\n",
    "    s = self.repr_func(obs)\n",
    "    return s \n",
    "\n",
    "\n",
    "class Prediction(hk.Module):\n",
    "  def __init__(self, num_actions, full_support_size, name='prediction'):\n",
    "    super().__init__(name=name)        \n",
    "    \n",
    "    self.v_func = hk.Sequential([\n",
    "        hk.Linear(16), jax.nn.elu,\n",
    "        hk.Linear(full_support_size)\n",
    "    ])\n",
    "    self.pi_func = hk.Sequential([\n",
    "        hk.Linear(16), jax.nn.elu,\n",
    "        hk.Linear(num_actions)\n",
    "    ])\n",
    "  \n",
    "  def __call__(self, s):\n",
    "    v = self.v_func(s)\n",
    "    logits = self.pi_func(s)\n",
    "    logits = jax.nn.softmax(logits, axis=-1)\n",
    "    return v, logits\n",
    "\n",
    "\n",
    "class Dynamic(hk.Module):\n",
    "  def __init__(self, embedding_dim, num_actions, full_support_size, name='dynamic'):\n",
    "    super().__init__(name=name)\n",
    "    \n",
    "    self.ns_func = hk.Sequential([\n",
    "        hk.Linear(16), jax.nn.elu,\n",
    "        hk.Linear(embedding_dim)\n",
    "    ])\n",
    "    self.r_func = hk.Sequential([\n",
    "        hk.Linear(16), jax.nn.elu,\n",
    "        hk.Linear(full_support_size)\n",
    "    ])\n",
    "    self.cat_func = jax.jit(lambda s, a: \n",
    "                            jnp.concatenate([s, jax.nn.one_hot(a, num_actions)],\n",
    "                                            axis=1)\n",
    "                            )\n",
    "  \n",
    "  def __call__(self, s, a):\n",
    "    sa = self.cat_func(s, a)\n",
    "    r = self.r_func(sa)\n",
    "    ns = self.ns_func(sa)\n",
    "    return r, ns\n",
    "\n",
    "\n",
    "def init_representation_func(representation_module, embedding_dim):\n",
    "    def representation_func(obs):\n",
    "      repr_model = representation_module(embedding_dim)\n",
    "      return repr_model(obs)\n",
    "    return representation_func\n",
    "  \n",
    "def init_prediction_func(prediction_module, num_actions, full_support_size):\n",
    "  def prediction_func(s):\n",
    "    pred_model = prediction_module(num_actions, full_support_size)\n",
    "    return pred_model(s)\n",
    "  return prediction_func\n",
    "\n",
    "def init_dynamic_func(dynamic_module, embedding_dim, num_actions, full_support_size):\n",
    "  def dynamic_func(s, a):\n",
    "    dy_model = dynamic_module(embedding_dim, num_actions, full_support_size)\n",
    "    return dy_model(s, a)\n",
    "  return dynamic_func \n",
    "\n",
    "repr_fn = init_representation_func(Representation, embedding_size)\n",
    "pred_fn = init_prediction_func(Prediction, 2, full_support_size)\n",
    "dy_fn = init_dynamic_func(Dynamic, embedding_size, 2, full_support_size)  \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`muax` has `Episode tracer` and `replay buffuer` to track and store trajectories from interacting with environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer = muax.PNStep(10, discount, 0.5)\n",
    "buffer = muax.TrajectoryReplayBuffer(500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`muax` leverages `optax` to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_transform = muax.model.optimizer(init_value=0.02, peak_value=0.02, end_value=0.002, warmup_steps=5000, transition_steps=5000)\n",
    "# gradient_transform = optax.adam(0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer warm up stage...\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TrainMonitor:ep: 1,\tT: 9,\tG: 8,\tavg_r: 1,\tavg_G: 8,\tt: 8,\tdt: 2443.100ms,\tv: 0.0416,\tRn: 4.4,\tloss: 4.32,\ttraining_step: 7,\ttest_G: 8.8\n",
      "INFO:TrainMonitor:ep: 2,\tT: 18,\tG: 8,\tavg_r: 1,\tavg_G: 8,\tt: 8,\tdt: 1725.632ms,\tv: 5.42,\tRn: 4.4,\tloss: 2.11,\ttraining_step: 14\n",
      "INFO:TrainMonitor:ep: 3,\tT: 39,\tG: 20,\tavg_r: 1,\tavg_G: 12,\tt: 20,\tdt: 460.329ms,\tv: 7.01,\tRn: 10.7,\tloss: 1.88,\ttraining_step: 33\n",
      "INFO:TrainMonitor:ep: 4,\tT: 49,\tG: 9,\tavg_r: 1,\tavg_G: 11.2,\tt: 9,\tdt: 13.108ms,\tv: 8.9,\tRn: 4.87,\tloss: 1.77,\ttraining_step: 41\n",
      "INFO:TrainMonitor:ep: 5,\tT: 59,\tG: 9,\tavg_r: 1,\tavg_G: 10.8,\tt: 9,\tdt: 13.046ms,\tv: 7.71,\tRn: 4.87,\tloss: 1.75,\ttraining_step: 49\n",
      "INFO:TrainMonitor:ep: 6,\tT: 79,\tG: 19,\tavg_r: 1,\tavg_G: 12.2,\tt: 19,\tdt: 160.603ms,\tv: 8.71,\tRn: 10.7,\tloss: 1.72,\ttraining_step: 67\n",
      "INFO:TrainMonitor:ep: 7,\tT: 100,\tG: 20,\tavg_r: 1,\tavg_G: 13.3,\tt: 20,\tdt: 12.565ms,\tv: 9.77,\tRn: 11.8,\tloss: 1.66,\ttraining_step: 86\n",
      "INFO:TrainMonitor:ep: 8,\tT: 114,\tG: 13,\tavg_r: 1,\tavg_G: 13.2,\tt: 13,\tdt: 14.322ms,\tv: 10.2,\tRn: 8.35,\tloss: 1.74,\ttraining_step: 98\n",
      "INFO:TrainMonitor:ep: 9,\tT: 123,\tG: 8,\tavg_r: 1,\tavg_G: 12.7,\tt: 8,\tdt: 14.548ms,\tv: 9.11,\tRn: 4.4,\tloss: 1.72,\ttraining_step: 105\n",
      "INFO:TrainMonitor:ep: 10,\tT: 152,\tG: 28,\tavg_r: 1,\tavg_G: 14.2,\tt: 28,\tdt: 115.821ms,\tv: 9.8,\tRn: 13.6,\tloss: 1.74,\ttraining_step: 132\n",
      "INFO:TrainMonitor:ep: 11,\tT: 169,\tG: 16,\tavg_r: 1,\tavg_G: 14.4,\tt: 16,\tdt: 17.914ms,\tv: 10.5,\tRn: 10.4,\tloss: 1.75,\ttraining_step: 147,\ttest_G: 9.4\n",
      "INFO:TrainMonitor:ep: 12,\tT: 186,\tG: 16,\tavg_r: 1,\tavg_G: 14.5,\tt: 16,\tdt: 389.451ms,\tv: 10.6,\tRn: 10.4,\tloss: 1.73,\ttraining_step: 162\n",
      "INFO:TrainMonitor:ep: 13,\tT: 211,\tG: 24,\tavg_r: 1,\tavg_G: 15.5,\tt: 24,\tdt: 13.746ms,\tv: 10.4,\tRn: 13.2,\tloss: 1.78,\ttraining_step: 185\n",
      "INFO:TrainMonitor:ep: 14,\tT: 221,\tG: 9,\tavg_r: 1,\tavg_G: 14.8,\tt: 9,\tdt: 13.632ms,\tv: 9.27,\tRn: 4.87,\tloss: 1.72,\ttraining_step: 193\n",
      "INFO:TrainMonitor:ep: 15,\tT: 230,\tG: 8,\tavg_r: 1,\tavg_G: 14.2,\tt: 8,\tdt: 13.485ms,\tv: 8.71,\tRn: 4.4,\tloss: 1.76,\ttraining_step: 200\n",
      "INFO:TrainMonitor:ep: 16,\tT: 244,\tG: 13,\tavg_r: 1,\tavg_G: 14,\tt: 13,\tdt: 13.456ms,\tv: 10.8,\tRn: 8.5,\tloss: 1.74,\ttraining_step: 212\n",
      "INFO:TrainMonitor:ep: 17,\tT: 276,\tG: 31,\tavg_r: 1,\tavg_G: 15.7,\tt: 31,\tdt: 13.723ms,\tv: 10,\tRn: 14.1,\tloss: 1.79,\ttraining_step: 242\n",
      "INFO:TrainMonitor:ep: 18,\tT: 286,\tG: 9,\tavg_r: 1,\tavg_G: 15.1,\tt: 9,\tdt: 13.432ms,\tv: 9.77,\tRn: 4.87,\tloss: 1.77,\ttraining_step: 250\n",
      "INFO:TrainMonitor:ep: 19,\tT: 302,\tG: 15,\tavg_r: 1,\tavg_G: 15.1,\tt: 15,\tdt: 13.555ms,\tv: 9.9,\tRn: 9.53,\tloss: 1.8,\ttraining_step: 264\n",
      "INFO:TrainMonitor:ep: 20,\tT: 336,\tG: 33,\tavg_r: 1,\tavg_G: 16.9,\tt: 33,\tdt: 14.202ms,\tv: 10.3,\tRn: 14.2,\tloss: 1.82,\ttraining_step: 296\n",
      "INFO:TrainMonitor:ep: 21,\tT: 355,\tG: 18,\tavg_r: 1,\tavg_G: 17,\tt: 18,\tdt: 22.551ms,\tv: 12.7,\tRn: 12.3,\tloss: 1.84,\ttraining_step: 313,\ttest_G: 19.9\n",
      "INFO:TrainMonitor:ep: 22,\tT: 368,\tG: 12,\tavg_r: 1,\tavg_G: 16.5,\tt: 12,\tdt: 13.351ms,\tv: 12.3,\tRn: 7.84,\tloss: 1.83,\ttraining_step: 324\n",
      "INFO:TrainMonitor:ep: 23,\tT: 388,\tG: 19,\tavg_r: 1,\tavg_G: 16.7,\tt: 19,\tdt: 165.703ms,\tv: 13.4,\tRn: 13,\tloss: 1.84,\ttraining_step: 342\n",
      "INFO:TrainMonitor:ep: 24,\tT: 428,\tG: 39,\tavg_r: 1,\tavg_G: 18.9,\tt: 39,\tdt: 13.606ms,\tv: 10.7,\tRn: 15.2,\tloss: 1.86,\ttraining_step: 380\n",
      "INFO:TrainMonitor:ep: 25,\tT: 452,\tG: 23,\tavg_r: 1,\tavg_G: 19.4,\tt: 23,\tdt: 13.973ms,\tv: 12.6,\tRn: 14,\tloss: 1.88,\ttraining_step: 402\n",
      "INFO:TrainMonitor:ep: 26,\tT: 475,\tG: 22,\tavg_r: 1,\tavg_G: 19.6,\tt: 22,\tdt: 13.285ms,\tv: 12.6,\tRn: 13.4,\tloss: 1.91,\ttraining_step: 423\n",
      "INFO:TrainMonitor:ep: 27,\tT: 491,\tG: 15,\tavg_r: 1,\tavg_G: 19.2,\tt: 15,\tdt: 13.127ms,\tv: 12.5,\tRn: 10.2,\tloss: 1.93,\ttraining_step: 437\n",
      "INFO:TrainMonitor:ep: 28,\tT: 514,\tG: 22,\tavg_r: 1,\tavg_G: 19.4,\tt: 22,\tdt: 13.310ms,\tv: 12.8,\tRn: 13.8,\tloss: 1.89,\ttraining_step: 458\n",
      "INFO:TrainMonitor:ep: 29,\tT: 528,\tG: 13,\tavg_r: 1,\tavg_G: 18.8,\tt: 13,\tdt: 12.951ms,\tv: 15.8,\tRn: 9.39,\tloss: 1.94,\ttraining_step: 470\n",
      "INFO:TrainMonitor:ep: 30,\tT: 540,\tG: 11,\tavg_r: 1,\tavg_G: 18,\tt: 11,\tdt: 12.907ms,\tv: 12.5,\tRn: 6.71,\tloss: 1.92,\ttraining_step: 480\n",
      "INFO:TrainMonitor:ep: 31,\tT: 554,\tG: 13,\tavg_r: 1,\tavg_G: 17.5,\tt: 13,\tdt: 18.760ms,\tv: 13.8,\tRn: 9.11,\tloss: 1.98,\ttraining_step: 492,\ttest_G: 9.1\n",
      "INFO:TrainMonitor:ep: 32,\tT: 571,\tG: 16,\tavg_r: 1,\tavg_G: 17.4,\tt: 16,\tdt: 13.331ms,\tv: 15.4,\tRn: 12.3,\tloss: 1.92,\ttraining_step: 507\n",
      "INFO:TrainMonitor:ep: 33,\tT: 603,\tG: 31,\tavg_r: 1,\tavg_G: 18.7,\tt: 31,\tdt: 13.148ms,\tv: 15,\tRn: 17.3,\tloss: 1.87,\ttraining_step: 537\n",
      "INFO:TrainMonitor:ep: 34,\tT: 620,\tG: 16,\tavg_r: 1,\tavg_G: 18.5,\tt: 16,\tdt: 12.913ms,\tv: 17.5,\tRn: 13.3,\tloss: 1.97,\ttraining_step: 552\n",
      "INFO:TrainMonitor:ep: 35,\tT: 634,\tG: 13,\tavg_r: 1,\tavg_G: 17.9,\tt: 13,\tdt: 12.733ms,\tv: 13.2,\tRn: 8.74,\tloss: 1.95,\ttraining_step: 564\n",
      "INFO:TrainMonitor:ep: 36,\tT: 668,\tG: 33,\tavg_r: 1,\tavg_G: 19.4,\tt: 33,\tdt: 13.224ms,\tv: 15.8,\tRn: 18,\tloss: 1.89,\ttraining_step: 596\n",
      "INFO:TrainMonitor:ep: 37,\tT: 690,\tG: 21,\tavg_r: 1,\tavg_G: 19.6,\tt: 21,\tdt: 13.688ms,\tv: 15.6,\tRn: 15,\tloss: 1.99,\ttraining_step: 616\n",
      "INFO:TrainMonitor:ep: 38,\tT: 705,\tG: 14,\tavg_r: 1,\tavg_G: 19,\tt: 14,\tdt: 12.681ms,\tv: 13.6,\tRn: 10.3,\tloss: 1.97,\ttraining_step: 629\n",
      "INFO:TrainMonitor:ep: 39,\tT: 718,\tG: 12,\tavg_r: 1,\tavg_G: 18.3,\tt: 12,\tdt: 12.713ms,\tv: 15.4,\tRn: 8.41,\tloss: 1.85,\ttraining_step: 640\n",
      "INFO:TrainMonitor:ep: 40,\tT: 749,\tG: 30,\tavg_r: 1,\tavg_G: 19.5,\tt: 30,\tdt: 13.075ms,\tv: 10.2,\tRn: 13.3,\tloss: 1.92,\ttraining_step: 669\n",
      "INFO:TrainMonitor:ep: 41,\tT: 761,\tG: 11,\tavg_r: 1,\tavg_G: 18.6,\tt: 11,\tdt: 23.092ms,\tv: 14.3,\tRn: 7.02,\tloss: 1.92,\ttraining_step: 679,\ttest_G: 13.1\n",
      "INFO:TrainMonitor:ep: 42,\tT: 773,\tG: 11,\tavg_r: 1,\tavg_G: 17.9,\tt: 11,\tdt: 12.851ms,\tv: 16.1,\tRn: 7.21,\tloss: 1.89,\ttraining_step: 689\n",
      "INFO:TrainMonitor:ep: 43,\tT: 782,\tG: 8,\tavg_r: 1,\tavg_G: 16.9,\tt: 8,\tdt: 12.387ms,\tv: 13.1,\tRn: 4.4,\tloss: 1.9,\ttraining_step: 696\n",
      "INFO:TrainMonitor:ep: 44,\tT: 805,\tG: 22,\tavg_r: 1,\tavg_G: 17.4,\tt: 22,\tdt: 12.941ms,\tv: 15.8,\tRn: 15.3,\tloss: 1.9,\ttraining_step: 717\n",
      "INFO:TrainMonitor:ep: 45,\tT: 828,\tG: 22,\tavg_r: 1,\tavg_G: 17.9,\tt: 22,\tdt: 13.158ms,\tv: 15.7,\tRn: 15.2,\tloss: 1.91,\ttraining_step: 738\n",
      "INFO:TrainMonitor:ep: 46,\tT: 874,\tG: 45,\tavg_r: 1,\tavg_G: 20.6,\tt: 45,\tdt: 13.068ms,\tv: 13.4,\tRn: 17.3,\tloss: 1.94,\ttraining_step: 782\n",
      "INFO:TrainMonitor:ep: 47,\tT: 888,\tG: 13,\tavg_r: 1,\tavg_G: 19.8,\tt: 13,\tdt: 12.921ms,\tv: 15.3,\tRn: 10.1,\tloss: 1.97,\ttraining_step: 794\n",
      "INFO:TrainMonitor:ep: 48,\tT: 904,\tG: 15,\tavg_r: 1,\tavg_G: 19.3,\tt: 15,\tdt: 12.896ms,\tv: 19,\tRn: 12.7,\tloss: 1.95,\ttraining_step: 808\n",
      "INFO:TrainMonitor:ep: 49,\tT: 915,\tG: 10,\tavg_r: 1,\tavg_G: 18.4,\tt: 10,\tdt: 324.717ms,\tv: 14,\tRn: 5.34,\tloss: 2,\ttraining_step: 817\n",
      "INFO:TrainMonitor:ep: 50,\tT: 929,\tG: 13,\tavg_r: 1,\tavg_G: 17.9,\tt: 13,\tdt: 13.211ms,\tv: 17.1,\tRn: 9.79,\tloss: 1.99,\ttraining_step: 829\n",
      "INFO:TrainMonitor:ep: 51,\tT: 950,\tG: 20,\tavg_r: 1,\tavg_G: 18.1,\tt: 20,\tdt: 18.928ms,\tv: 17.6,\tRn: 15.7,\tloss: 2.05,\ttraining_step: 848,\ttest_G: 12.5\n",
      "INFO:TrainMonitor:ep: 52,\tT: 962,\tG: 11,\tavg_r: 1,\tavg_G: 17.4,\tt: 11,\tdt: 12.909ms,\tv: 15.7,\tRn: 6.97,\tloss: 1.88,\ttraining_step: 858\n",
      "INFO:TrainMonitor:ep: 53,\tT: 990,\tG: 27,\tavg_r: 1,\tavg_G: 18.3,\tt: 27,\tdt: 13.364ms,\tv: 17.2,\tRn: 18.1,\tloss: 1.99,\ttraining_step: 884\n",
      "INFO:TrainMonitor:ep: 54,\tT: 1,001,\tG: 10,\tavg_r: 1,\tavg_G: 17.5,\tt: 10,\tdt: 12.949ms,\tv: 16.3,\tRn: 5.34,\tloss: 1.93,\ttraining_step: 893\n",
      "INFO:TrainMonitor:ep: 55,\tT: 1,024,\tG: 22,\tavg_r: 1,\tavg_G: 17.9,\tt: 22,\tdt: 13.269ms,\tv: 18,\tRn: 17.2,\tloss: 1.97,\ttraining_step: 914\n",
      "INFO:TrainMonitor:ep: 56,\tT: 1,040,\tG: 15,\tavg_r: 1,\tavg_G: 17.7,\tt: 15,\tdt: 12.844ms,\tv: 15.1,\tRn: 10.3,\tloss: 1.98,\ttraining_step: 928\n",
      "INFO:TrainMonitor:ep: 57,\tT: 1,056,\tG: 15,\tavg_r: 1,\tavg_G: 17.4,\tt: 15,\tdt: 12.993ms,\tv: 21.7,\tRn: 13.4,\tloss: 2.02,\ttraining_step: 942\n",
      "INFO:TrainMonitor:ep: 58,\tT: 1,068,\tG: 11,\tavg_r: 1,\tavg_G: 16.7,\tt: 11,\tdt: 12.644ms,\tv: 18.9,\tRn: 7.41,\tloss: 1.95,\ttraining_step: 952\n",
      "INFO:TrainMonitor:ep: 59,\tT: 1,120,\tG: 51,\tavg_r: 1,\tavg_G: 20.2,\tt: 51,\tdt: 13.369ms,\tv: 19.9,\tRn: 23.5,\tloss: 2.01,\ttraining_step: 1e+03\n",
      "INFO:TrainMonitor:ep: 60,\tT: 1,142,\tG: 21,\tavg_r: 1,\tavg_G: 20.3,\tt: 21,\tdt: 13.311ms,\tv: 22.5,\tRn: 18.9,\tloss: 2,\ttraining_step: 1.02e+03\n",
      "INFO:TrainMonitor:ep: 61,\tT: 1,153,\tG: 10,\tavg_r: 1,\tavg_G: 19.2,\tt: 10,\tdt: 29.577ms,\tv: 18.3,\tRn: 5.34,\tloss: 2.05,\ttraining_step: 1.03e+03,\ttest_G: 18.7\n",
      "INFO:TrainMonitor:ep: 62,\tT: 1,188,\tG: 34,\tavg_r: 1,\tavg_G: 20.7,\tt: 34,\tdt: 99.923ms,\tv: 20.9,\tRn: 22.4,\tloss: 2.03,\ttraining_step: 1.06e+03\n",
      "INFO:TrainMonitor:ep: 63,\tT: 1,220,\tG: 31,\tavg_r: 1,\tavg_G: 21.7,\tt: 31,\tdt: 13.766ms,\tv: 11.8,\tRn: 14.4,\tloss: 2.04,\ttraining_step: 1.09e+03\n",
      "INFO:TrainMonitor:ep: 64,\tT: 1,236,\tG: 15,\tavg_r: 1,\tavg_G: 21.1,\tt: 15,\tdt: 13.541ms,\tv: 17,\tRn: 12.1,\tloss: 2.02,\ttraining_step: 1.11e+03\n",
      "INFO:TrainMonitor:ep: 65,\tT: 1,256,\tG: 19,\tavg_r: 1,\tavg_G: 20.9,\tt: 19,\tdt: 13.292ms,\tv: 16.5,\tRn: 13.6,\tloss: 2.01,\ttraining_step: 1.13e+03\n",
      "INFO:TrainMonitor:ep: 66,\tT: 1,268,\tG: 11,\tavg_r: 1,\tavg_G: 19.9,\tt: 11,\tdt: 12.969ms,\tv: 17.3,\tRn: 7.11,\tloss: 2.07,\ttraining_step: 1.14e+03\n",
      "INFO:TrainMonitor:ep: 67,\tT: 1,280,\tG: 11,\tavg_r: 1,\tavg_G: 19,\tt: 11,\tdt: 13.230ms,\tv: 20.2,\tRn: 7.42,\tloss: 2.08,\ttraining_step: 1.15e+03\n",
      "INFO:TrainMonitor:ep: 68,\tT: 1,331,\tG: 50,\tavg_r: 1,\tavg_G: 22.1,\tt: 50,\tdt: 69.824ms,\tv: 11.6,\tRn: 16.5,\tloss: 2.09,\ttraining_step: 1.2e+03\n",
      "INFO:TrainMonitor:ep: 69,\tT: 1,374,\tG: 42,\tavg_r: 1,\tavg_G: 24.1,\tt: 42,\tdt: 13.779ms,\tv: 12.1,\tRn: 16.1,\tloss: 2.09,\ttraining_step: 1.24e+03\n",
      "INFO:TrainMonitor:ep: 70,\tT: 1,416,\tG: 41,\tavg_r: 1,\tavg_G: 25.8,\tt: 41,\tdt: 13.935ms,\tv: 21.4,\tRn: 23.9,\tloss: 2.06,\ttraining_step: 1.28e+03\n",
      "INFO:TrainMonitor:ep: 71,\tT: 1,432,\tG: 15,\tavg_r: 1,\tavg_G: 24.7,\tt: 15,\tdt: 100.206ms,\tv: 19.8,\tRn: 14.1,\tloss: 2.07,\ttraining_step: 1.29e+03,\ttest_G: 167\n",
      "INFO:TrainMonitor:ep: 72,\tT: 1,447,\tG: 14,\tavg_r: 1,\tavg_G: 23.6,\tt: 14,\tdt: 12.960ms,\tv: 14.8,\tRn: 9.41,\tloss: 2.11,\ttraining_step: 1.3e+03\n",
      "INFO:TrainMonitor:ep: 73,\tT: 1,460,\tG: 12,\tavg_r: 1,\tavg_G: 22.5,\tt: 12,\tdt: 13.442ms,\tv: 13.4,\tRn: 7.73,\tloss: 2.09,\ttraining_step: 1.31e+03\n",
      "INFO:TrainMonitor:ep: 74,\tT: 1,489,\tG: 28,\tavg_r: 1,\tavg_G: 23,\tt: 28,\tdt: 13.602ms,\tv: 21.5,\tRn: 20.4,\tloss: 2.07,\ttraining_step: 1.34e+03\n",
      "INFO:TrainMonitor:ep: 75,\tT: 1,519,\tG: 29,\tavg_r: 1,\tavg_G: 23.6,\tt: 29,\tdt: 13.449ms,\tv: 22.1,\tRn: 22.7,\tloss: 2.07,\ttraining_step: 1.37e+03\n",
      "INFO:TrainMonitor:ep: 76,\tT: 1,531,\tG: 11,\tavg_r: 1,\tavg_G: 22.4,\tt: 11,\tdt: 13.329ms,\tv: 18.7,\tRn: 7.41,\tloss: 2.06,\ttraining_step: 1.38e+03\n",
      "INFO:TrainMonitor:ep: 77,\tT: 1,543,\tG: 11,\tavg_r: 1,\tavg_G: 21.2,\tt: 11,\tdt: 13.080ms,\tv: 13.8,\tRn: 6.61,\tloss: 2.07,\ttraining_step: 1.39e+03\n",
      "INFO:TrainMonitor:ep: 78,\tT: 1,556,\tG: 12,\tavg_r: 1,\tavg_G: 20.3,\tt: 12,\tdt: 15.415ms,\tv: 20.8,\tRn: 9.5,\tloss: 2.1,\ttraining_step: 1.4e+03\n",
      "INFO:TrainMonitor:ep: 79,\tT: 1,581,\tG: 24,\tavg_r: 1,\tavg_G: 20.7,\tt: 24,\tdt: 28.457ms,\tv: 21.5,\tRn: 20.3,\tloss: 2.12,\ttraining_step: 1.42e+03\n",
      "INFO:TrainMonitor:ep: 80,\tT: 1,626,\tG: 44,\tavg_r: 1,\tavg_G: 23,\tt: 44,\tdt: 29.819ms,\tv: 14.3,\tRn: 18.3,\tloss: 2.17,\ttraining_step: 1.47e+03\n",
      "INFO:TrainMonitor:ep: 81,\tT: 1,638,\tG: 11,\tavg_r: 1,\tavg_G: 21.8,\tt: 11,\tdt: 53.184ms,\tv: 13.5,\tRn: 6.57,\tloss: 2.07,\ttraining_step: 1.48e+03,\ttest_G: 21.6\n",
      "INFO:TrainMonitor:ep: 82,\tT: 1,656,\tG: 17,\tavg_r: 1,\tavg_G: 21.3,\tt: 17,\tdt: 28.929ms,\tv: 14.9,\tRn: 12,\tloss: 2.1,\ttraining_step: 1.49e+03\n",
      "INFO:TrainMonitor:ep: 83,\tT: 1,674,\tG: 17,\tavg_r: 1,\tavg_G: 20.9,\tt: 17,\tdt: 29.415ms,\tv: 12.9,\tRn: 11.2,\tloss: 2.13,\ttraining_step: 1.51e+03\n",
      "INFO:TrainMonitor:ep: 84,\tT: 1,688,\tG: 13,\tavg_r: 1,\tavg_G: 20.1,\tt: 13,\tdt: 17.802ms,\tv: 13.2,\tRn: 8.62,\tloss: 2.15,\ttraining_step: 1.52e+03\n",
      "INFO:TrainMonitor:ep: 85,\tT: 1,699,\tG: 10,\tavg_r: 1,\tavg_G: 19.1,\tt: 10,\tdt: 13.286ms,\tv: 21.8,\tRn: 5.34,\tloss: 2.11,\ttraining_step: 1.53e+03\n",
      "INFO:TrainMonitor:ep: 86,\tT: 1,712,\tG: 12,\tavg_r: 1,\tavg_G: 18.4,\tt: 12,\tdt: 13.447ms,\tv: 14.9,\tRn: 7.84,\tloss: 2.13,\ttraining_step: 1.54e+03\n",
      "INFO:TrainMonitor:ep: 87,\tT: 1,726,\tG: 13,\tavg_r: 1,\tavg_G: 17.8,\tt: 13,\tdt: 13.336ms,\tv: 20.8,\tRn: 11.4,\tloss: 2.18,\ttraining_step: 1.55e+03\n",
      "INFO:TrainMonitor:ep: 88,\tT: 1,748,\tG: 21,\tavg_r: 1,\tavg_G: 18.2,\tt: 21,\tdt: 13.406ms,\tv: 24.6,\tRn: 20,\tloss: 2.1,\ttraining_step: 1.57e+03\n",
      "INFO:TrainMonitor:ep: 89,\tT: 1,767,\tG: 18,\tavg_r: 1,\tavg_G: 18.1,\tt: 18,\tdt: 13.599ms,\tv: 23.6,\tRn: 17.9,\tloss: 2.1,\ttraining_step: 1.59e+03\n",
      "INFO:TrainMonitor:ep: 90,\tT: 1,783,\tG: 15,\tavg_r: 1,\tavg_G: 17.8,\tt: 15,\tdt: 13.125ms,\tv: 13.9,\tRn: 10.1,\tloss: 2.14,\ttraining_step: 1.6e+03\n",
      "INFO:TrainMonitor:ep: 91,\tT: 1,798,\tG: 14,\tavg_r: 1,\tavg_G: 17.4,\tt: 14,\tdt: 34.599ms,\tv: 22.9,\tRn: 13,\tloss: 2.12,\ttraining_step: 1.62e+03,\ttest_G: 37.1\n",
      "INFO:TrainMonitor:ep: 92,\tT: 1,817,\tG: 18,\tavg_r: 1,\tavg_G: 17.5,\tt: 18,\tdt: 13.361ms,\tv: 22.9,\tRn: 17.5,\tloss: 2.11,\ttraining_step: 1.63e+03\n",
      "INFO:TrainMonitor:ep: 93,\tT: 1,852,\tG: 34,\tavg_r: 1,\tavg_G: 19.2,\tt: 34,\tdt: 13.673ms,\tv: 15.1,\tRn: 16.7,\tloss: 2.18,\ttraining_step: 1.67e+03\n",
      "INFO:TrainMonitor:ep: 94,\tT: 1,886,\tG: 33,\tavg_r: 1,\tavg_G: 20.5,\tt: 33,\tdt: 13.580ms,\tv: 21.6,\tRn: 22.5,\tloss: 2.14,\ttraining_step: 1.7e+03\n",
      "INFO:TrainMonitor:ep: 95,\tT: 1,901,\tG: 14,\tavg_r: 1,\tavg_G: 19.9,\tt: 14,\tdt: 13.388ms,\tv: 22.3,\tRn: 12.7,\tloss: 2.14,\ttraining_step: 1.71e+03\n",
      "INFO:TrainMonitor:ep: 96,\tT: 1,929,\tG: 27,\tavg_r: 1,\tavg_G: 20.6,\tt: 27,\tdt: 13.724ms,\tv: 13.8,\tRn: 15.5,\tloss: 2.14,\ttraining_step: 1.74e+03\n",
      "INFO:TrainMonitor:ep: 97,\tT: 1,944,\tG: 14,\tavg_r: 1,\tavg_G: 19.9,\tt: 14,\tdt: 13.454ms,\tv: 22.8,\tRn: 13.2,\tloss: 2.19,\ttraining_step: 1.75e+03\n",
      "INFO:TrainMonitor:ep: 98,\tT: 2,035,\tG: 90,\tavg_r: 1,\tavg_G: 26.9,\tt: 90,\tdt: 14.032ms,\tv: 13.5,\tRn: 19.7,\tloss: 2.17,\ttraining_step: 1.84e+03\n",
      "INFO:TrainMonitor:ep: 99,\tT: 2,047,\tG: 11,\tavg_r: 1,\tavg_G: 25.3,\tt: 11,\tdt: 13.642ms,\tv: 22.4,\tRn: 7.62,\tloss: 2.18,\ttraining_step: 1.85e+03\n",
      "INFO:TrainMonitor:ep: 100,\tT: 2,092,\tG: 44,\tavg_r: 1,\tavg_G: 27.2,\tt: 44,\tdt: 13.916ms,\tv: 12.1,\tRn: 16.7,\tloss: 2.16,\ttraining_step: 1.89e+03\n",
      "INFO:TrainMonitor:ep: 101,\tT: 2,127,\tG: 34,\tavg_r: 1,\tavg_G: 27.9,\tt: 34,\tdt: 33.058ms,\tv: 11.2,\tRn: 14.7,\tloss: 2.16,\ttraining_step: 1.92e+03,\ttest_G: 80.2\n",
      "INFO:TrainMonitor:ep: 102,\tT: 2,136,\tG: 8,\tavg_r: 1,\tavg_G: 25.9,\tt: 8,\tdt: 13.211ms,\tv: 20.2,\tRn: 4.4,\tloss: 2.16,\ttraining_step: 1.93e+03\n",
      "INFO:TrainMonitor:ep: 103,\tT: 2,157,\tG: 20,\tavg_r: 1,\tavg_G: 25.3,\tt: 20,\tdt: 14.844ms,\tv: 21.2,\tRn: 17.8,\tloss: 2.18,\ttraining_step: 1.95e+03\n",
      "INFO:TrainMonitor:ep: 104,\tT: 2,177,\tG: 19,\tavg_r: 1,\tavg_G: 24.7,\tt: 19,\tdt: 14.611ms,\tv: 21.9,\tRn: 17.5,\tloss: 2.22,\ttraining_step: 1.97e+03\n",
      "INFO:TrainMonitor:ep: 105,\tT: 2,193,\tG: 15,\tavg_r: 1,\tavg_G: 23.7,\tt: 15,\tdt: 34.911ms,\tv: 22.1,\tRn: 14,\tloss: 2.16,\ttraining_step: 1.98e+03\n",
      "INFO:TrainMonitor:ep: 106,\tT: 2,214,\tG: 20,\tavg_r: 1,\tavg_G: 23.3,\tt: 20,\tdt: 13.863ms,\tv: 22.2,\tRn: 18.4,\tloss: 2.17,\ttraining_step: 2e+03\n",
      "INFO:TrainMonitor:ep: 107,\tT: 2,229,\tG: 14,\tavg_r: 1,\tavg_G: 22.4,\tt: 14,\tdt: 14.215ms,\tv: 25,\tRn: 13.4,\tloss: 2.23,\ttraining_step: 2.02e+03\n",
      "INFO:TrainMonitor:ep: 108,\tT: 2,247,\tG: 17,\tavg_r: 1,\tavg_G: 21.9,\tt: 17,\tdt: 14.140ms,\tv: 16.8,\tRn: 12.8,\tloss: 2.14,\ttraining_step: 2.03e+03\n",
      "INFO:TrainMonitor:ep: 109,\tT: 2,274,\tG: 26,\tavg_r: 1,\tavg_G: 22.3,\tt: 26,\tdt: 13.845ms,\tv: 22,\tRn: 21.1,\tloss: 2.15,\ttraining_step: 2.06e+03\n",
      "INFO:TrainMonitor:ep: 110,\tT: 2,303,\tG: 28,\tavg_r: 1,\tavg_G: 22.9,\tt: 28,\tdt: 13.992ms,\tv: 22,\tRn: 21.2,\tloss: 2.18,\ttraining_step: 2.08e+03\n",
      "INFO:TrainMonitor:ep: 111,\tT: 2,317,\tG: 13,\tavg_r: 1,\tavg_G: 21.9,\tt: 13,\tdt: 26.500ms,\tv: 22.2,\tRn: 11.6,\tloss: 2.22,\ttraining_step: 2.1e+03,\ttest_G: 20.5\n",
      "INFO:TrainMonitor:ep: 112,\tT: 2,341,\tG: 23,\tavg_r: 1,\tavg_G: 22,\tt: 23,\tdt: 13.607ms,\tv: 20.2,\tRn: 19.5,\tloss: 2.21,\ttraining_step: 2.12e+03\n",
      "INFO:TrainMonitor:ep: 113,\tT: 2,643,\tG: 301,\tavg_r: 1,\tavg_G: 49.9,\tt: 301,\tdt: 33.654ms,\tv: 12.7,\tRn: 20.4,\tloss: 2.21,\ttraining_step: 2.42e+03\n",
      "INFO:TrainMonitor:ep: 114,\tT: 2,665,\tG: 21,\tavg_r: 1,\tavg_G: 47,\tt: 21,\tdt: 14.121ms,\tv: 25,\tRn: 19.9,\tloss: 2.2,\ttraining_step: 2.44e+03\n",
      "INFO:TrainMonitor:ep: 115,\tT: 2,675,\tG: 9,\tavg_r: 1,\tavg_G: 43.2,\tt: 9,\tdt: 13.954ms,\tv: 20.8,\tRn: 4.87,\tloss: 2.24,\ttraining_step: 2.44e+03\n",
      "INFO:TrainMonitor:ep: 116,\tT: 2,692,\tG: 16,\tavg_r: 1,\tavg_G: 40.5,\tt: 16,\tdt: 14.191ms,\tv: 17.3,\tRn: 11.8,\tloss: 2.21,\ttraining_step: 2.46e+03\n",
      "INFO:TrainMonitor:ep: 117,\tT: 2,725,\tG: 32,\tavg_r: 1,\tavg_G: 39.6,\tt: 32,\tdt: 14.244ms,\tv: 12.5,\tRn: 15,\tloss: 2.22,\ttraining_step: 2.49e+03\n",
      "INFO:TrainMonitor:ep: 118,\tT: 2,827,\tG: 101,\tavg_r: 1,\tavg_G: 45.8,\tt: 101,\tdt: 14.637ms,\tv: 16,\tRn: 22.3,\tloss: 2.21,\ttraining_step: 2.59e+03\n",
      "INFO:TrainMonitor:ep: 119,\tT: 2,841,\tG: 13,\tavg_r: 1,\tavg_G: 42.5,\tt: 13,\tdt: 14.148ms,\tv: 23.8,\tRn: 11.6,\tloss: 2.23,\ttraining_step: 2.6e+03\n",
      "INFO:TrainMonitor:ep: 120,\tT: 2,859,\tG: 17,\tavg_r: 1,\tavg_G: 39.9,\tt: 17,\tdt: 14.196ms,\tv: 23.9,\tRn: 16.9,\tloss: 2.25,\ttraining_step: 2.62e+03\n",
      "INFO:TrainMonitor:ep: 121,\tT: 2,871,\tG: 11,\tavg_r: 1,\tavg_G: 37,\tt: 11,\tdt: 53.668ms,\tv: 22.8,\tRn: 7.85,\tloss: 2.26,\ttraining_step: 2.63e+03,\ttest_G: 54.4\n",
      "INFO:TrainMonitor:ep: 122,\tT: 2,883,\tG: 11,\tavg_r: 1,\tavg_G: 34.4,\tt: 11,\tdt: 14.474ms,\tv: 22.9,\tRn: 7.72,\tloss: 2.27,\ttraining_step: 2.64e+03\n",
      "INFO:TrainMonitor:ep: 123,\tT: 2,898,\tG: 14,\tavg_r: 1,\tavg_G: 32.4,\tt: 14,\tdt: 14.177ms,\tv: 21.1,\tRn: 12.8,\tloss: 2.26,\ttraining_step: 2.65e+03\n",
      "INFO:TrainMonitor:ep: 124,\tT: 2,943,\tG: 44,\tavg_r: 1,\tavg_G: 33.6,\tt: 44,\tdt: 14.182ms,\tv: 15.9,\tRn: 19.1,\tloss: 2.2,\ttraining_step: 2.7e+03\n",
      "INFO:TrainMonitor:ep: 125,\tT: 3,073,\tG: 129,\tavg_r: 1,\tavg_G: 43.1,\tt: 129,\tdt: 14.576ms,\tv: 15.7,\tRn: 22.4,\tloss: 2.23,\ttraining_step: 2.82e+03\n",
      "INFO:TrainMonitor:ep: 126,\tT: 3,091,\tG: 17,\tavg_r: 1,\tavg_G: 40.5,\tt: 17,\tdt: 14.272ms,\tv: 17.3,\tRn: 12.6,\tloss: 2.23,\ttraining_step: 2.84e+03\n",
      "INFO:TrainMonitor:ep: 127,\tT: 3,106,\tG: 14,\tavg_r: 1,\tavg_G: 37.8,\tt: 14,\tdt: 13.909ms,\tv: 22.2,\tRn: 12.9,\tloss: 2.22,\ttraining_step: 2.85e+03\n",
      "INFO:TrainMonitor:ep: 128,\tT: 3,229,\tG: 122,\tavg_r: 1,\tavg_G: 46.3,\tt: 122,\tdt: 14.766ms,\tv: 12.8,\tRn: 19.7,\tloss: 2.23,\ttraining_step: 2.97e+03\n",
      "INFO:TrainMonitor:ep: 129,\tT: 3,249,\tG: 19,\tavg_r: 1,\tavg_G: 43.5,\tt: 19,\tdt: 14.096ms,\tv: 21.4,\tRn: 17.4,\tloss: 2.25,\ttraining_step: 2.99e+03\n",
      "INFO:TrainMonitor:ep: 130,\tT: 3,261,\tG: 11,\tavg_r: 1,\tavg_G: 40.3,\tt: 11,\tdt: 13.470ms,\tv: 21.3,\tRn: 7.64,\tloss: 2.22,\ttraining_step: 3e+03\n",
      "INFO:TrainMonitor:ep: 131,\tT: 3,283,\tG: 21,\tavg_r: 1,\tavg_G: 38.4,\tt: 21,\tdt: 39.057ms,\tv: 22.5,\tRn: 18.8,\tloss: 2.21,\ttraining_step: 3.02e+03,\ttest_G: 65.8\n",
      "INFO:TrainMonitor:ep: 132,\tT: 3,302,\tG: 18,\tavg_r: 1,\tavg_G: 36.3,\tt: 18,\tdt: 14.038ms,\tv: 22.9,\tRn: 17.1,\tloss: 2.22,\ttraining_step: 3.04e+03\n",
      "INFO:TrainMonitor:ep: 133,\tT: 3,342,\tG: 39,\tavg_r: 1,\tavg_G: 36.6,\tt: 39,\tdt: 14.148ms,\tv: 22.4,\tRn: 23.7,\tloss: 2.22,\ttraining_step: 3.08e+03\n",
      "INFO:TrainMonitor:ep: 134,\tT: 3,355,\tG: 12,\tavg_r: 1,\tavg_G: 34.1,\tt: 12,\tdt: 13.382ms,\tv: 21.6,\tRn: 9.71,\tloss: 2.2,\ttraining_step: 3.09e+03\n",
      "INFO:TrainMonitor:ep: 135,\tT: 3,378,\tG: 22,\tavg_r: 1,\tavg_G: 32.9,\tt: 22,\tdt: 13.977ms,\tv: 25.3,\tRn: 20.9,\tloss: 2.22,\ttraining_step: 3.11e+03\n",
      "INFO:TrainMonitor:ep: 136,\tT: 3,401,\tG: 22,\tavg_r: 1,\tavg_G: 31.8,\tt: 22,\tdt: 14.208ms,\tv: 25.4,\tRn: 21,\tloss: 2.25,\ttraining_step: 3.13e+03\n",
      "INFO:TrainMonitor:ep: 137,\tT: 3,598,\tG: 196,\tavg_r: 1,\tavg_G: 48.2,\tt: 196,\tdt: 16.414ms,\tv: 11.7,\tRn: 19.1,\tloss: 2.26,\ttraining_step: 3.32e+03\n",
      "INFO:TrainMonitor:ep: 138,\tT: 3,701,\tG: 102,\tavg_r: 1,\tavg_G: 53.6,\tt: 102,\tdt: 14.809ms,\tv: 10.9,\tRn: 17.6,\tloss: 2.25,\ttraining_step: 3.42e+03\n",
      "INFO:TrainMonitor:ep: 139,\tT: 3,798,\tG: 96,\tavg_r: 1,\tavg_G: 57.9,\tt: 96,\tdt: 14.894ms,\tv: 16.3,\tRn: 21.9,\tloss: 2.27,\ttraining_step: 3.52e+03\n",
      "INFO:TrainMonitor:ep: 140,\tT: 3,816,\tG: 17,\tavg_r: 1,\tavg_G: 53.8,\tt: 17,\tdt: 14.695ms,\tv: 22.9,\tRn: 16.4,\tloss: 2.25,\ttraining_step: 3.54e+03\n",
      "INFO:TrainMonitor:ep: 141,\tT: 4,123,\tG: 306,\tavg_r: 1,\tavg_G: 79,\tt: 306,\tdt: 16.444ms,\tv: 19.7,\tRn: 26.6,\tloss: 2.24,\ttraining_step: 3.84e+03,\ttest_G: 43.1\n",
      "INFO:TrainMonitor:ep: 142,\tT: 4,163,\tG: 39,\tavg_r: 1,\tavg_G: 75,\tt: 39,\tdt: 14.961ms,\tv: 24,\tRn: 25.7,\tloss: 2.29,\ttraining_step: 3.88e+03\n",
      "INFO:TrainMonitor:ep: 143,\tT: 4,184,\tG: 20,\tavg_r: 1,\tavg_G: 69.5,\tt: 20,\tdt: 14.643ms,\tv: 24,\tRn: 19.3,\tloss: 2.22,\ttraining_step: 3.9e+03\n",
      "INFO:TrainMonitor:ep: 144,\tT: 4,202,\tG: 17,\tavg_r: 1,\tavg_G: 64.2,\tt: 17,\tdt: 16.014ms,\tv: 25.1,\tRn: 17.1,\tloss: 2.27,\ttraining_step: 3.91e+03\n",
      "INFO:TrainMonitor:ep: 145,\tT: 4,229,\tG: 26,\tavg_r: 1,\tavg_G: 60.4,\tt: 26,\tdt: 15.072ms,\tv: 25,\tRn: 22.6,\tloss: 2.26,\ttraining_step: 3.94e+03\n",
      "INFO:TrainMonitor:ep: 146,\tT: 4,248,\tG: 18,\tavg_r: 1,\tavg_G: 56.2,\tt: 18,\tdt: 16.129ms,\tv: 25.9,\tRn: 18.1,\tloss: 2.27,\ttraining_step: 3.96e+03\n",
      "INFO:TrainMonitor:ep: 147,\tT: 4,400,\tG: 151,\tavg_r: 1,\tavg_G: 65.7,\tt: 151,\tdt: 16.042ms,\tv: 21.9,\tRn: 27.7,\tloss: 2.26,\ttraining_step: 4.11e+03\n",
      "INFO:TrainMonitor:ep: 148,\tT: 4,420,\tG: 19,\tavg_r: 1,\tavg_G: 61,\tt: 19,\tdt: 15.144ms,\tv: 24.6,\tRn: 18.7,\tloss: 2.27,\ttraining_step: 4.12e+03\n",
      "INFO:TrainMonitor:ep: 149,\tT: 4,476,\tG: 55,\tavg_r: 1,\tavg_G: 60.4,\tt: 55,\tdt: 15.087ms,\tv: 24.5,\tRn: 27.1,\tloss: 2.29,\ttraining_step: 4.18e+03\n",
      "INFO:TrainMonitor:ep: 150,\tT: 4,532,\tG: 55,\tavg_r: 1,\tavg_G: 59.9,\tt: 55,\tdt: 15.215ms,\tv: 22.3,\tRn: 26.1,\tloss: 2.28,\ttraining_step: 4.23e+03\n",
      "INFO:TrainMonitor:ep: 151,\tT: 4,592,\tG: 59,\tavg_r: 1,\tavg_G: 59.8,\tt: 59,\tdt: 30.614ms,\tv: 25.7,\tRn: 28.6,\tloss: 2.27,\ttraining_step: 4.29e+03,\ttest_G: 120\n",
      "INFO:TrainMonitor:ep: 152,\tT: 4,622,\tG: 29,\tavg_r: 1,\tavg_G: 56.7,\tt: 29,\tdt: 14.445ms,\tv: 25.1,\tRn: 24.1,\tloss: 2.29,\ttraining_step: 4.32e+03\n",
      "INFO:TrainMonitor:ep: 153,\tT: 4,646,\tG: 23,\tavg_r: 1,\tavg_G: 53.3,\tt: 23,\tdt: 14.893ms,\tv: 25.9,\tRn: 22.1,\tloss: 2.26,\ttraining_step: 4.34e+03\n",
      "INFO:TrainMonitor:ep: 154,\tT: 4,670,\tG: 23,\tavg_r: 1,\tavg_G: 50.3,\tt: 23,\tdt: 14.705ms,\tv: 26.5,\tRn: 22.2,\tloss: 2.28,\ttraining_step: 4.36e+03\n",
      "INFO:TrainMonitor:ep: 155,\tT: 4,708,\tG: 37,\tavg_r: 1,\tavg_G: 49,\tt: 37,\tdt: 15.553ms,\tv: 26.6,\tRn: 26.8,\tloss: 2.26,\ttraining_step: 4.4e+03\n",
      "INFO:TrainMonitor:ep: 156,\tT: 4,746,\tG: 37,\tavg_r: 1,\tavg_G: 47.8,\tt: 37,\tdt: 14.932ms,\tv: 26.7,\tRn: 26.7,\tloss: 2.26,\ttraining_step: 4.43e+03\n",
      "INFO:TrainMonitor:ep: 157,\tT: 4,782,\tG: 35,\tavg_r: 1,\tavg_G: 46.5,\tt: 35,\tdt: 14.995ms,\tv: 26.3,\tRn: 26.9,\tloss: 2.24,\ttraining_step: 4.47e+03\n",
      "INFO:TrainMonitor:ep: 158,\tT: 4,817,\tG: 34,\tavg_r: 1,\tavg_G: 45.2,\tt: 34,\tdt: 15.342ms,\tv: 26.9,\tRn: 26.3,\tloss: 2.27,\ttraining_step: 4.5e+03\n",
      "INFO:TrainMonitor:ep: 159,\tT: 4,858,\tG: 40,\tavg_r: 1,\tavg_G: 44.7,\tt: 40,\tdt: 14.927ms,\tv: 27.5,\tRn: 27.9,\tloss: 2.27,\ttraining_step: 4.54e+03\n",
      "INFO:TrainMonitor:ep: 160,\tT: 4,883,\tG: 24,\tavg_r: 1,\tavg_G: 42.6,\tt: 24,\tdt: 14.849ms,\tv: 26.4,\tRn: 22.8,\tloss: 2.29,\ttraining_step: 4.56e+03\n",
      "INFO:TrainMonitor:ep: 161,\tT: 4,905,\tG: 21,\tavg_r: 1,\tavg_G: 40.5,\tt: 21,\tdt: 38.042ms,\tv: 28.1,\tRn: 21.5,\tloss: 2.31,\ttraining_step: 4.58e+03,\ttest_G: 64.1\n",
      "INFO:TrainMonitor:ep: 162,\tT: 4,923,\tG: 17,\tavg_r: 1,\tavg_G: 38.1,\tt: 17,\tdt: 14.786ms,\tv: 26.4,\tRn: 17.8,\tloss: 2.3,\ttraining_step: 4.6e+03\n",
      "INFO:TrainMonitor:ep: 163,\tT: 4,961,\tG: 37,\tavg_r: 1,\tavg_G: 38,\tt: 37,\tdt: 21.388ms,\tv: 26.3,\tRn: 27.5,\tloss: 2.34,\ttraining_step: 4.64e+03\n",
      "INFO:TrainMonitor:ep: 164,\tT: 5,008,\tG: 46,\tavg_r: 1,\tavg_G: 38.8,\tt: 46,\tdt: 32.449ms,\tv: 26,\tRn: 27.5,\tloss: 2.32,\ttraining_step: 4.68e+03\n",
      "INFO:TrainMonitor:ep: 165,\tT: 5,037,\tG: 28,\tavg_r: 1,\tavg_G: 37.7,\tt: 28,\tdt: 30.853ms,\tv: 26.8,\tRn: 24.7,\tloss: 2.32,\ttraining_step: 4.71e+03\n",
      "INFO:TrainMonitor:ep: 166,\tT: 5,091,\tG: 53,\tavg_r: 1,\tavg_G: 39.3,\tt: 53,\tdt: 24.192ms,\tv: 23.4,\tRn: 26.1,\tloss: 2.28,\ttraining_step: 4.76e+03\n",
      "INFO:TrainMonitor:ep: 167,\tT: 5,133,\tG: 41,\tavg_r: 1,\tavg_G: 39.4,\tt: 41,\tdt: 14.787ms,\tv: 28.9,\tRn: 29,\tloss: 2.32,\ttraining_step: 4.8e+03\n",
      "INFO:TrainMonitor:ep: 168,\tT: 5,154,\tG: 20,\tavg_r: 1,\tavg_G: 37.5,\tt: 20,\tdt: 15.017ms,\tv: 27.1,\tRn: 20.3,\tloss: 2.31,\ttraining_step: 4.82e+03\n",
      "INFO:TrainMonitor:ep: 169,\tT: 5,186,\tG: 31,\tavg_r: 1,\tavg_G: 36.8,\tt: 31,\tdt: 15.068ms,\tv: 26.8,\tRn: 25.7,\tloss: 2.32,\ttraining_step: 4.85e+03\n",
      "INFO:TrainMonitor:ep: 170,\tT: 5,217,\tG: 30,\tavg_r: 1,\tavg_G: 36.2,\tt: 30,\tdt: 15.066ms,\tv: 28.5,\tRn: 26,\tloss: 2.28,\ttraining_step: 4.88e+03\n",
      "INFO:TrainMonitor:ep: 171,\tT: 5,243,\tG: 25,\tavg_r: 1,\tavg_G: 35,\tt: 25,\tdt: 34.090ms,\tv: 26.8,\tRn: 23.5,\tloss: 2.3,\ttraining_step: 4.9e+03,\ttest_G: 62.6\n",
      "INFO:TrainMonitor:ep: 172,\tT: 5,279,\tG: 35,\tavg_r: 1,\tavg_G: 35,\tt: 35,\tdt: 15.153ms,\tv: 26.4,\tRn: 26.4,\tloss: 2.29,\ttraining_step: 4.94e+03\n",
      "INFO:TrainMonitor:ep: 173,\tT: 5,332,\tG: 52,\tavg_r: 1,\tavg_G: 36.7,\tt: 52,\tdt: 15.330ms,\tv: 28.3,\tRn: 30.2,\tloss: 2.29,\ttraining_step: 4.99e+03\n",
      "INFO:TrainMonitor:ep: 174,\tT: 5,355,\tG: 22,\tavg_r: 1,\tavg_G: 35.3,\tt: 22,\tdt: 14.776ms,\tv: 29.1,\tRn: 22.6,\tloss: 2.28,\ttraining_step: 5.01e+03\n",
      "INFO:TrainMonitor:ep: 175,\tT: 5,394,\tG: 38,\tavg_r: 1,\tavg_G: 35.5,\tt: 38,\tdt: 93.800ms,\tv: 25.9,\tRn: 25.8,\tloss: 2.3,\ttraining_step: 5.04e+03\n",
      "INFO:TrainMonitor:ep: 176,\tT: 5,428,\tG: 33,\tavg_r: 1,\tavg_G: 35.3,\tt: 33,\tdt: 15.954ms,\tv: 28.2,\tRn: 26.5,\tloss: 2.34,\ttraining_step: 5.08e+03\n",
      "INFO:TrainMonitor:ep: 177,\tT: 5,472,\tG: 43,\tavg_r: 1,\tavg_G: 36.1,\tt: 43,\tdt: 15.057ms,\tv: 28.2,\tRn: 28.9,\tloss: 2.33,\ttraining_step: 5.12e+03\n",
      "INFO:TrainMonitor:ep: 178,\tT: 5,499,\tG: 26,\tavg_r: 1,\tavg_G: 35,\tt: 26,\tdt: 14.748ms,\tv: 28.4,\tRn: 24.5,\tloss: 2.28,\ttraining_step: 5.14e+03\n",
      "INFO:TrainMonitor:ep: 179,\tT: 5,523,\tG: 23,\tavg_r: 1,\tavg_G: 33.8,\tt: 23,\tdt: 14.803ms,\tv: 28.2,\tRn: 23.2,\tloss: 2.28,\ttraining_step: 5.16e+03\n",
      "INFO:TrainMonitor:ep: 180,\tT: 5,552,\tG: 28,\tavg_r: 1,\tavg_G: 33.3,\tt: 28,\tdt: 14.962ms,\tv: 28.1,\tRn: 25.2,\tloss: 2.29,\ttraining_step: 5.19e+03\n",
      "INFO:TrainMonitor:ep: 181,\tT: 5,573,\tG: 20,\tavg_r: 1,\tavg_G: 31.9,\tt: 20,\tdt: 34.027ms,\tv: 28.4,\tRn: 21,\tloss: 2.29,\ttraining_step: 5.21e+03,\ttest_G: 49.2\n",
      "INFO:TrainMonitor:ep: 182,\tT: 5,612,\tG: 38,\tavg_r: 1,\tavg_G: 32.5,\tt: 38,\tdt: 15.518ms,\tv: 28.1,\tRn: 28.3,\tloss: 2.35,\ttraining_step: 5.25e+03\n",
      "INFO:TrainMonitor:ep: 183,\tT: 5,643,\tG: 30,\tavg_r: 1,\tavg_G: 32.3,\tt: 30,\tdt: 15.023ms,\tv: 28,\tRn: 25.7,\tloss: 2.3,\ttraining_step: 5.28e+03\n",
      "INFO:TrainMonitor:ep: 184,\tT: 5,673,\tG: 29,\tavg_r: 1,\tavg_G: 32,\tt: 29,\tdt: 14.929ms,\tv: 29.2,\tRn: 26.2,\tloss: 2.32,\ttraining_step: 5.30e+03\n",
      "INFO:TrainMonitor:ep: 185,\tT: 5,720,\tG: 46,\tavg_r: 1,\tavg_G: 33.4,\tt: 46,\tdt: 15.136ms,\tv: 28.4,\tRn: 30.2,\tloss: 2.32,\ttraining_step: 5.35e+03\n",
      "INFO:TrainMonitor:ep: 186,\tT: 5,754,\tG: 33,\tavg_r: 1,\tavg_G: 33.3,\tt: 33,\tdt: 15.164ms,\tv: 30.5,\tRn: 28.3,\tloss: 2.33,\ttraining_step: 5.38e+03\n",
      "INFO:TrainMonitor:ep: 187,\tT: 5,802,\tG: 47,\tavg_r: 1,\tavg_G: 34.7,\tt: 47,\tdt: 15.952ms,\tv: 29.3,\tRn: 30.2,\tloss: 2.35,\ttraining_step: 5.43e+03\n",
      "INFO:TrainMonitor:ep: 188,\tT: 5,829,\tG: 26,\tavg_r: 1,\tavg_G: 33.8,\tt: 26,\tdt: 15.213ms,\tv: 29.3,\tRn: 24.8,\tloss: 2.33,\ttraining_step: 5.45e+03\n",
      "INFO:TrainMonitor:ep: 189,\tT: 5,862,\tG: 32,\tavg_r: 1,\tavg_G: 33.6,\tt: 32,\tdt: 14.672ms,\tv: 30.4,\tRn: 27.5,\tloss: 2.32,\ttraining_step: 5.48e+03\n",
      "INFO:TrainMonitor:ep: 190,\tT: 5,902,\tG: 39,\tavg_r: 1,\tavg_G: 34.2,\tt: 39,\tdt: 15.705ms,\tv: 29.6,\tRn: 29.2,\tloss: 2.3,\ttraining_step: 5.52e+03\n",
      "INFO:TrainMonitor:ep: 191,\tT: 5,970,\tG: 67,\tavg_r: 1,\tavg_G: 37.5,\tt: 67,\tdt: 42.340ms,\tv: 29.5,\tRn: 32.3,\tloss: 2.32,\ttraining_step: 5.59e+03,\ttest_G: 218\n",
      "INFO:TrainMonitor:ep: 192,\tT: 6,012,\tG: 41,\tavg_r: 1,\tavg_G: 37.8,\tt: 41,\tdt: 15.268ms,\tv: 29.5,\tRn: 30,\tloss: 2.35,\ttraining_step: 5.63e+03\n",
      "INFO:TrainMonitor:ep: 193,\tT: 6,037,\tG: 24,\tavg_r: 1,\tavg_G: 36.4,\tt: 24,\tdt: 15.017ms,\tv: 28.5,\tRn: 23.7,\tloss: 2.28,\ttraining_step: 5.65e+03\n",
      "INFO:TrainMonitor:ep: 194,\tT: 6,071,\tG: 33,\tavg_r: 1,\tavg_G: 36.1,\tt: 33,\tdt: 24.991ms,\tv: 29,\tRn: 27.6,\tloss: 2.33,\ttraining_step: 5.68e+03\n",
      "INFO:TrainMonitor:ep: 195,\tT: 6,105,\tG: 33,\tavg_r: 1,\tavg_G: 35.8,\tt: 33,\tdt: 15.137ms,\tv: 30.7,\tRn: 28.1,\tloss: 2.34,\ttraining_step: 5.72e+03\n",
      "INFO:TrainMonitor:ep: 196,\tT: 6,157,\tG: 51,\tavg_r: 1,\tavg_G: 37.3,\tt: 51,\tdt: 15.214ms,\tv: 28.9,\tRn: 30.6,\tloss: 2.34,\ttraining_step: 5.76e+03\n",
      "INFO:TrainMonitor:ep: 197,\tT: 6,238,\tG: 80,\tavg_r: 1,\tavg_G: 41.6,\tt: 80,\tdt: 15.487ms,\tv: 31,\tRn: 34,\tloss: 2.33,\ttraining_step: 5.84e+03\n",
      "INFO:TrainMonitor:ep: 198,\tT: 6,282,\tG: 43,\tavg_r: 1,\tavg_G: 41.7,\tt: 43,\tdt: 15.012ms,\tv: 30.1,\tRn: 30.5,\tloss: 2.32,\ttraining_step: 5.89e+03\n",
      "INFO:TrainMonitor:ep: 199,\tT: 6,339,\tG: 56,\tavg_r: 1,\tavg_G: 43.1,\tt: 56,\tdt: 15.241ms,\tv: 31.7,\tRn: 33,\tloss: 2.31,\ttraining_step: 5.94e+03\n",
      "INFO:TrainMonitor:ep: 200,\tT: 6,391,\tG: 51,\tavg_r: 1,\tavg_G: 43.9,\tt: 51,\tdt: 15.204ms,\tv: 31.3,\tRn: 32.2,\tloss: 2.32,\ttraining_step: 5.99e+03\n",
      "INFO:TrainMonitor:ep: 201,\tT: 6,481,\tG: 89,\tavg_r: 1,\tavg_G: 48.4,\tt: 89,\tdt: 19.442ms,\tv: 32.2,\tRn: 35.5,\tloss: 2.34,\ttraining_step: 6.08e+03,\ttest_G: 48.7\n",
      "INFO:TrainMonitor:ep: 202,\tT: 6,524,\tG: 42,\tavg_r: 1,\tavg_G: 47.8,\tt: 42,\tdt: 15.131ms,\tv: 30.4,\tRn: 30.4,\tloss: 2.35,\ttraining_step: 6.12e+03\n",
      "INFO:TrainMonitor:ep: 203,\tT: 6,621,\tG: 96,\tavg_r: 1,\tavg_G: 52.6,\tt: 96,\tdt: 15.343ms,\tv: 31.1,\tRn: 34.9,\tloss: 2.33,\ttraining_step: 6.22e+03\n",
      "INFO:TrainMonitor:ep: 204,\tT: 6,687,\tG: 65,\tavg_r: 1,\tavg_G: 53.9,\tt: 65,\tdt: 15.649ms,\tv: 31.2,\tRn: 34,\tloss: 2.35,\ttraining_step: 6.28e+03\n",
      "INFO:TrainMonitor:ep: 205,\tT: 6,733,\tG: 45,\tavg_r: 1,\tavg_G: 53,\tt: 45,\tdt: 15.156ms,\tv: 30.9,\tRn: 31.1,\tloss: 2.34,\ttraining_step: 6.32e+03\n",
      "INFO:TrainMonitor:ep: 206,\tT: 7,196,\tG: 462,\tavg_r: 1,\tavg_G: 93.9,\tt: 462,\tdt: 15.955ms,\tv: 17.1,\tRn: 24.5,\tloss: 2.34,\ttraining_step: 6.78e+03\n",
      "INFO:TrainMonitor:ep: 207,\tT: 7,319,\tG: 122,\tavg_r: 1,\tavg_G: 96.7,\tt: 122,\tdt: 15.831ms,\tv: 31.8,\tRn: 36.3,\tloss: 2.33,\ttraining_step: 6.90e+03\n",
      "INFO:TrainMonitor:ep: 208,\tT: 7,360,\tG: 40,\tavg_r: 1,\tavg_G: 91,\tt: 40,\tdt: 15.947ms,\tv: 29.4,\tRn: 29.3,\tloss: 2.37,\ttraining_step: 6.94e+03\n",
      "INFO:TrainMonitor:ep: 209,\tT: 7,423,\tG: 62,\tavg_r: 1,\tavg_G: 88.1,\tt: 62,\tdt: 15.645ms,\tv: 32.7,\tRn: 34.5,\tloss: 2.35,\ttraining_step: 7.00e+03\n",
      "INFO:TrainMonitor:ep: 210,\tT: 7,493,\tG: 69,\tavg_r: 1,\tavg_G: 86.2,\tt: 69,\tdt: 16.006ms,\tv: 32,\tRn: 34.6,\tloss: 2.32,\ttraining_step: 7.07e+03\n",
      "INFO:TrainMonitor:ep: 211,\tT: 7,546,\tG: 52,\tavg_r: 1,\tavg_G: 82.8,\tt: 52,\tdt: 24.155ms,\tv: 33,\tRn: 33.6,\tloss: 2.34,\ttraining_step: 7.12e+03,\ttest_G: 56.1\n",
      "INFO:TrainMonitor:ep: 212,\tT: 7,584,\tG: 37,\tavg_r: 1,\tavg_G: 78.2,\tt: 37,\tdt: 16.725ms,\tv: 31.6,\tRn: 30.5,\tloss: 2.35,\ttraining_step: 7.16e+03\n",
      "INFO:TrainMonitor:ep: 213,\tT: 7,776,\tG: 191,\tavg_r: 1,\tavg_G: 89.5,\tt: 191,\tdt: 16.386ms,\tv: 32.9,\tRn: 37.7,\tloss: 2.35,\ttraining_step: 7.35e+03\n",
      "INFO:TrainMonitor:ep: 214,\tT: 7,810,\tG: 33,\tavg_r: 1,\tavg_G: 83.8,\tt: 33,\tdt: 16.174ms,\tv: 32.5,\tRn: 29.8,\tloss: 2.29,\ttraining_step: 7.38e+03\n",
      "INFO:TrainMonitor:ep: 215,\tT: 7,905,\tG: 94,\tavg_r: 1,\tavg_G: 84.9,\tt: 94,\tdt: 16.418ms,\tv: 33.4,\tRn: 37,\tloss: 2.33,\ttraining_step: 7.48e+03\n",
      "INFO:TrainMonitor:ep: 216,\tT: 7,962,\tG: 56,\tavg_r: 1,\tavg_G: 82,\tt: 56,\tdt: 15.939ms,\tv: 31.6,\tRn: 33.2,\tloss: 2.37,\ttraining_step: 7.53e+03\n",
      "INFO:TrainMonitor:ep: 217,\tT: 8,463,\tG: 500,\tavg_r: 1,\tavg_G: 124,\tt: 500,\tdt: 26.950ms,\tv: 20.5,\tRn: 29.5,\tloss: 2.34,\ttraining_step: 8.03e+03\n",
      "INFO:TrainMonitor:ep: 218,\tT: 8,619,\tG: 155,\tavg_r: 1,\tavg_G: 127,\tt: 155,\tdt: 17.072ms,\tv: 33.3,\tRn: 37.9,\tloss: 2.34,\ttraining_step: 8.18e+03\n",
      "INFO:TrainMonitor:ep: 219,\tT: 9,038,\tG: 418,\tavg_r: 1,\tavg_G: 156,\tt: 418,\tdt: 17.957ms,\tv: 24.9,\tRn: 31.4,\tloss: 2.35,\ttraining_step: 8.6e+03\n",
      "INFO:TrainMonitor:ep: 220,\tT: 9,142,\tG: 103,\tavg_r: 1,\tavg_G: 151,\tt: 103,\tdt: 16.661ms,\tv: 33.1,\tRn: 36.8,\tloss: 2.34,\ttraining_step: 8.7e+03\n",
      "INFO:TrainMonitor:ep: 221,\tT: 9,207,\tG: 64,\tavg_r: 1,\tavg_G: 142,\tt: 64,\tdt: 52.504ms,\tv: 33.4,\tRn: 35.2,\tloss: 2.32,\ttraining_step: 8.76e+03,\ttest_G: 288\n",
      "INFO:TrainMonitor:ep: 222,\tT: 9,571,\tG: 363,\tavg_r: 1,\tavg_G: 164,\tt: 363,\tdt: 17.909ms,\tv: 28.3,\tRn: 34.3,\tloss: 2.35,\ttraining_step: 9.13e+03\n",
      "INFO:TrainMonitor:ep: 223,\tT: 9,703,\tG: 131,\tavg_r: 1,\tavg_G: 161,\tt: 131,\tdt: 17.350ms,\tv: 35,\tRn: 38.9,\tloss: 2.35,\ttraining_step: 9.26e+03\n",
      "INFO:TrainMonitor:ep: 224,\tT: 9,880,\tG: 176,\tavg_r: 1,\tavg_G: 162,\tt: 176,\tdt: 17.485ms,\tv: 25.9,\tRn: 31.5,\tloss: 2.35,\ttraining_step: 9.43e+03\n",
      "INFO:TrainMonitor:ep: 225,\tT: 10,046,\tG: 165,\tavg_r: 1,\tavg_G: 163,\tt: 165,\tdt: 17.172ms,\tv: 35.3,\tRn: 39.7,\tloss: 2.34,\ttraining_step: 9.6e+03\n"
     ]
    }
   ],
   "source": [
    "model = muax.MuZero(repr_fn, pred_fn, dy_fn, policy='muzero', discount=discount,\n",
    "                    optimizer=gradient_transform, support_size=support_size)\n",
    "\n",
    "model_path = muax.fit(model, 'CartPole-v1', \n",
    "                    max_episodes=1000,\n",
    "                    max_training_steps=10000,\n",
    "                    tracer=tracer,\n",
    "                    buffer=buffer,\n",
    "                    k_steps=10,\n",
    "                    sample_per_trajectory=1,\n",
    "                    num_trajectory=32,\n",
    "                    tensorboard_dir='/home/fangbowen/tensorboard/cartpole',\n",
    "                    model_save_path='/home/fangbowen/models/cartpole',\n",
    "                    save_name='cartpole_model_params',\n",
    "                    random_seed=0,\n",
    "                    log_all_metrics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/fangbowen/models/epoch_0220_test_G_287.80000000/cartpole_model_params'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = muax.MuZero(repr_fn, pred_fn, dy_fn, policy='muzero', discount=discount,\n",
    "                    optimizer=gradient_transform, support_size=support_size)\n",
    "\n",
    "model.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.24"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from muax.test import test\n",
    "env_id = 'CartPole-v1'\n",
    "test_env = gym.make(env_id, render_mode='rgb_array')\n",
    "test_key = jax.random.PRNGKey(0)\n",
    "test(model, test_env, test_key, num_simulations=50, num_test_episodes=100, random_seed=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Customize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Customize the neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`muax` uses `haiku` to implement the neural networks. A tutorial for using `haiku` can be found at (link). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp \n",
    "import haiku as hk\n",
    "\n",
    "\n",
    "class Representation(hk.Module):\n",
    "  def __init__(self, embedding_dim, name='representation'):\n",
    "    super().__init__(name=name)\n",
    "\n",
    "    self.repr_func = hk.Sequential([\n",
    "        hk.Linear(embedding_dim), jax.nn.elu\n",
    "    ])\n",
    "\n",
    "  def __call__(self, obs):\n",
    "    s = self.repr_func(obs)\n",
    "    return s \n",
    "\n",
    "\n",
    "class Prediction(hk.Module):\n",
    "  def __init__(self, num_actions, full_support_size, name='prediction'):\n",
    "    super().__init__(name=name)        \n",
    "    \n",
    "    self.v_func = hk.Sequential([\n",
    "        hk.Linear(16), jax.nn.elu,\n",
    "        hk.Linear(full_support_size)\n",
    "    ])\n",
    "    self.pi_func = hk.Sequential([\n",
    "        hk.Linear(16), jax.nn.elu,\n",
    "        hk.Linear(num_actions)\n",
    "    ])\n",
    "  \n",
    "  def __call__(self, s):\n",
    "    v = self.v_func(s)\n",
    "    logits = self.pi_func(s)\n",
    "    logits = jax.nn.softmax(logits, axis=-1)\n",
    "    return v, logits\n",
    "\n",
    "\n",
    "class Dynamic(hk.Module):\n",
    "  def __init__(self, embedding_dim, num_actions, full_support_size, name='dynamic'):\n",
    "    super().__init__(name=name)\n",
    "    \n",
    "    self.ns_func = hk.Sequential([\n",
    "        hk.Linear(16), jax.nn.elu,\n",
    "        hk.Linear(embedding_dim)\n",
    "    ])\n",
    "    self.r_func = hk.Sequential([\n",
    "        hk.Linear(16), jax.nn.elu,\n",
    "        hk.Linear(full_support_size)\n",
    "    ])\n",
    "    self.cat_func = jax.jit(lambda s, a: \n",
    "                            jnp.concatenate([s, jax.nn.one_hot(a, num_actions)],\n",
    "                                            axis=1)\n",
    "                            )\n",
    "  \n",
    "  def __call__(self, s, a):\n",
    "    sa = self.cat_func(s, a)\n",
    "    r = self.r_func(sa)\n",
    "    ns = self.ns_func(sa)\n",
    "    return r, ns\n",
    "\n",
    "\n",
    "def init_representation_func(representation_module, embedding_dim):\n",
    "    def representation_func(obs):\n",
    "      repr_model = representation_module(embedding_dim)\n",
    "      return repr_model(obs)\n",
    "    return representation_func\n",
    "  \n",
    "def init_prediction_func(prediction_module, num_actions, full_support_size):\n",
    "  def prediction_func(s):\n",
    "    pred_model = prediction_module(num_actions, full_support_size)\n",
    "    return pred_model(s)\n",
    "  return prediction_func\n",
    "\n",
    "def init_dynamic_func(dynamic_module, embedding_dim, num_actions, full_support_size):\n",
    "  def dynamic_func(s, a):\n",
    "    dy_model = dynamic_module(embedding_dim, num_actions, full_support_size)\n",
    "    return dy_model(s, a)\n",
    "  return dynamic_func \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_size = 10 \n",
    "embedding_size = 8\n",
    "full_support_size = int(support_size * 2 + 1)\n",
    "repr_fn = init_representation_func(Representation, embedding_size)\n",
    "pred_fn = init_prediction_func(Prediction, 2, full_support_size)\n",
    "dy_fn = init_dynamic_func(Dynamic, embedding_size, 2, full_support_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Customize the training loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inside the `muax.fit` function, the main structure is a typical RL interacting loop. Reset the env, agent takes an action based on the observation, updated current state until done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from muax import Trajectory\n",
    "\n",
    "def temperature_fn(max_training_steps, training_steps):\n",
    "  if training_steps < 0.5 * max_training_steps:\n",
    "      return 1.0\n",
    "  elif training_steps < 0.75 * max_training_steps:\n",
    "      return 0.5\n",
    "  else:\n",
    "      return 0.25\n",
    "  \n",
    "def test(model, env, key, num_simulations, num_test_episodes=10, random_seed=None):\n",
    "    total_rewards = np.zeros(num_test_episodes)\n",
    "    for episode in range(num_test_episodes):\n",
    "        obs, info = env.reset(seed=random_seed)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        for t in range(env.spec.max_episode_steps):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            a = model.act(subkey, obs, \n",
    "                          with_pi=False, \n",
    "                          with_value=False, \n",
    "                          obs_from_batch=False,\n",
    "                          num_simulations=num_simulations,\n",
    "                          temperature=0.) # Use deterministic actions during testing\n",
    "            obs_next, r, done, truncated, info = env.step(a)\n",
    "            episode_reward += r\n",
    "            if done or truncated:\n",
    "                break \n",
    "            obs = obs_next \n",
    "        \n",
    "        total_rewards[episode] = episode_reward\n",
    "\n",
    "    average_test_reward = np.mean(total_rewards)\n",
    "    return average_test_reward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer warm up stage...\n",
      "start training...\n",
      "epoch: 0000, loss: 4.30174676, training_step: 8\n",
      "epoch: 0000, test_G: 9.20000000\n",
      "epoch: 0001, loss: 2.33003187, training_step: 15\n",
      "epoch: 0001, test_G: 9.20000000\n",
      "epoch: 0002, loss: 2.13727483, training_step: 22\n",
      "epoch: 0002, test_G: 9.10000000\n",
      "epoch: 0003, loss: 2.19564544, training_step: 31\n",
      "epoch: 0003, test_G: 10.00000000\n",
      "epoch: 0004, loss: 2.18947538, training_step: 38\n",
      "epoch: 0004, test_G: 9.20000000\n",
      "epoch: 0005, loss: 2.07392795, training_step: 45\n",
      "epoch: 0005, test_G: 9.20000000\n",
      "epoch: 0006, loss: 2.12085823, training_step: 55\n",
      "epoch: 0006, test_G: 10.10000000\n",
      "epoch: 0007, loss: 2.04124657, training_step: 62\n",
      "epoch: 0007, test_G: 21.10000000\n",
      "epoch: 0008, loss: 2.10093344, training_step: 85\n",
      "epoch: 0008, test_G: 9.40000000\n",
      "epoch: 0009, loss: 2.07039206, training_step: 96\n",
      "epoch: 0009, test_G: 9.30000000\n",
      "epoch: 0010, loss: 1.95028911, training_step: 103\n",
      "epoch: 0010, test_G: 32.60000000\n",
      "epoch: 0011, loss: 2.03606952, training_step: 117\n",
      "epoch: 0011, test_G: 14.00000000\n",
      "epoch: 0012, loss: 1.91113210, training_step: 127\n",
      "epoch: 0012, test_G: 15.90000000\n",
      "epoch: 0013, loss: 2.04753208, training_step: 135\n",
      "epoch: 0013, test_G: 9.70000000\n",
      "epoch: 0014, loss: 2.00741746, training_step: 152\n",
      "epoch: 0014, test_G: 11.70000000\n",
      "epoch: 0015, loss: 2.07170577, training_step: 164\n",
      "epoch: 0015, test_G: 14.30000000\n",
      "epoch: 0016, loss: 2.06664935, training_step: 180\n",
      "epoch: 0016, test_G: 9.30000000\n",
      "epoch: 0017, loss: 1.95452843, training_step: 190\n",
      "epoch: 0017, test_G: 21.80000000\n",
      "epoch: 0018, loss: 2.14966189, training_step: 247\n",
      "epoch: 0018, test_G: 10.30000000\n",
      "epoch: 0019, loss: 2.09843373, training_step: 260\n",
      "epoch: 0019, test_G: 11.70000000\n",
      "epoch: 0020, loss: 2.12216403, training_step: 270\n",
      "epoch: 0020, test_G: 16.90000000\n",
      "epoch: 0021, loss: 2.08357812, training_step: 281\n",
      "epoch: 0021, test_G: 20.50000000\n",
      "epoch: 0022, loss: 2.05192058, training_step: 288\n",
      "epoch: 0022, test_G: 33.10000000\n",
      "epoch: 0023, loss: 2.20395488, training_step: 403\n",
      "epoch: 0023, test_G: 12.40000000\n",
      "epoch: 0024, loss: 2.15664206, training_step: 413\n",
      "epoch: 0024, test_G: 71.20000000\n",
      "epoch: 0025, loss: 2.24740532, training_step: 529\n",
      "epoch: 0025, test_G: 12.00000000\n",
      "epoch: 0026, loss: 2.10795618, training_step: 540\n",
      "epoch: 0026, test_G: 25.90000000\n",
      "epoch: 0027, loss: 2.13733982, training_step: 554\n",
      "epoch: 0027, test_G: 11.50000000\n",
      "epoch: 0028, loss: 2.18890904, training_step: 603\n",
      "epoch: 0028, test_G: 70.40000000\n",
      "epoch: 0029, loss: 2.24500170, training_step: 684\n",
      "epoch: 0029, test_G: 35.50000000\n",
      "epoch: 0030, loss: 2.23469146, training_step: 738\n",
      "epoch: 0030, test_G: 53.80000000\n",
      "epoch: 0031, loss: 2.27101988, training_step: 799\n",
      "epoch: 0031, test_G: 74.50000000\n",
      "epoch: 0032, loss: 2.30447610, training_step: 963\n",
      "epoch: 0032, test_G: 25.80000000\n",
      "epoch: 0033, loss: 2.22010396, training_step: 988\n",
      "epoch: 0033, test_G: 40.80000000\n",
      "epoch: 0034, loss: 2.31058420, training_step: 1060\n",
      "epoch: 0034, test_G: 14.90000000\n",
      "epoch: 0035, loss: 2.20653380, training_step: 1075\n",
      "epoch: 0035, test_G: 179.40000000\n",
      "epoch: 0036, loss: 2.29692643, training_step: 1209\n",
      "epoch: 0036, test_G: 208.90000000\n",
      "epoch: 0037, loss: 2.37277783, training_step: 1229\n",
      "epoch: 0037, test_G: 16.50000000\n",
      "epoch: 0038, loss: 2.41986537, training_step: 1245\n",
      "epoch: 0038, test_G: 15.30000000\n",
      "epoch: 0039, loss: 2.32605819, training_step: 1258\n",
      "epoch: 0039, test_G: 24.20000000\n",
      "epoch: 0040, loss: 2.34046863, training_step: 1299\n",
      "epoch: 0040, test_G: 36.40000000\n",
      "epoch: 0041, loss: 2.35422051, training_step: 1329\n",
      "epoch: 0041, test_G: 28.80000000\n",
      "epoch: 0042, loss: 2.29816675, training_step: 1372\n",
      "epoch: 0042, test_G: 26.90000000\n",
      "epoch: 0043, loss: 2.39640515, training_step: 1398\n",
      "epoch: 0043, test_G: 47.40000000\n",
      "epoch: 0044, loss: 2.43474710, training_step: 1427\n",
      "epoch: 0044, test_G: 47.60000000\n",
      "epoch: 0045, loss: 2.44715658, training_step: 1502\n",
      "epoch: 0045, test_G: 21.00000000\n",
      "epoch: 0046, loss: 2.38833117, training_step: 1511\n",
      "epoch: 0046, test_G: 46.80000000\n",
      "epoch: 0047, loss: 2.32825931, training_step: 1520\n",
      "epoch: 0047, test_G: 15.80000000\n",
      "epoch: 0048, loss: 2.39865826, training_step: 1532\n",
      "epoch: 0048, test_G: 52.10000000\n",
      "epoch: 0049, loss: 2.40781941, training_step: 1546\n",
      "epoch: 0049, test_G: 31.10000000\n",
      "epoch: 0050, loss: 2.47004068, training_step: 1564\n",
      "epoch: 0050, test_G: 25.40000000\n",
      "epoch: 0051, loss: 2.42897020, training_step: 1576\n",
      "epoch: 0051, test_G: 23.80000000\n",
      "epoch: 0052, loss: 2.40151111, training_step: 1587\n",
      "epoch: 0052, test_G: 82.70000000\n",
      "epoch: 0053, loss: 2.45860044, training_step: 1620\n",
      "epoch: 0053, test_G: 54.40000000\n",
      "epoch: 0054, loss: 2.43068831, training_step: 1645\n",
      "epoch: 0054, test_G: 77.80000000\n",
      "epoch: 0055, loss: 2.41601428, training_step: 1742\n",
      "epoch: 0055, test_G: 42.10000000\n",
      "epoch: 0056, loss: 2.37207770, training_step: 1755\n",
      "epoch: 0056, test_G: 21.40000000\n",
      "epoch: 0057, loss: 2.42878320, training_step: 1797\n",
      "epoch: 0057, test_G: 28.20000000\n",
      "epoch: 0058, loss: 2.44305662, training_step: 1810\n",
      "epoch: 0058, test_G: 16.60000000\n",
      "epoch: 0059, loss: 2.41712057, training_step: 1841\n",
      "epoch: 0059, test_G: 42.60000000\n",
      "epoch: 0060, loss: 2.47585392, training_step: 1866\n",
      "epoch: 0060, test_G: 51.80000000\n",
      "epoch: 0061, loss: 2.49435365, training_step: 1886\n",
      "epoch: 0061, test_G: 41.30000000\n",
      "epoch: 0062, loss: 2.49217727, training_step: 1899\n",
      "epoch: 0062, test_G: 38.80000000\n",
      "epoch: 0063, loss: 2.48043504, training_step: 1991\n",
      "epoch: 0063, test_G: 39.10000000\n",
      "epoch: 0064, loss: 2.49684823, training_step: 2005\n",
      "epoch: 0064, test_G: 11.40000000\n",
      "epoch: 0065, loss: 2.45468513, training_step: 2023\n",
      "epoch: 0065, test_G: 45.50000000\n",
      "epoch: 0066, loss: 2.45429952, training_step: 2076\n",
      "epoch: 0066, test_G: 29.00000000\n",
      "epoch: 0067, loss: 2.47994489, training_step: 2123\n",
      "epoch: 0067, test_G: 26.10000000\n",
      "epoch: 0068, loss: 2.45930100, training_step: 2155\n",
      "epoch: 0068, test_G: 16.20000000\n",
      "epoch: 0069, loss: 2.49286164, training_step: 2189\n",
      "epoch: 0069, test_G: 9.50000000\n",
      "epoch: 0070, loss: 2.51763924, training_step: 2204\n",
      "epoch: 0070, test_G: 19.40000000\n",
      "epoch: 0071, loss: 2.53560491, training_step: 2222\n",
      "epoch: 0071, test_G: 37.20000000\n",
      "epoch: 0072, loss: 2.46645791, training_step: 2243\n",
      "epoch: 0072, test_G: 20.80000000\n",
      "epoch: 0073, loss: 2.52636757, training_step: 2290\n",
      "epoch: 0073, test_G: 14.90000000\n",
      "epoch: 0074, loss: 2.42490831, training_step: 2308\n",
      "epoch: 0074, test_G: 16.40000000\n",
      "epoch: 0075, loss: 2.47646549, training_step: 2333\n",
      "epoch: 0075, test_G: 14.80000000\n",
      "epoch: 0076, loss: 2.52322518, training_step: 2361\n",
      "epoch: 0076, test_G: 19.90000000\n",
      "epoch: 0077, loss: 2.48482329, training_step: 2411\n",
      "epoch: 0077, test_G: 49.90000000\n",
      "epoch: 0078, loss: 2.51185157, training_step: 2452\n",
      "epoch: 0078, test_G: 36.70000000\n",
      "epoch: 0079, loss: 2.51244020, training_step: 2466\n",
      "epoch: 0079, test_G: 52.80000000\n",
      "epoch: 0080, loss: 2.52778172, training_step: 2478\n",
      "epoch: 0080, test_G: 9.50000000\n",
      "epoch: 0081, loss: 2.50877767, training_step: 2532\n",
      "epoch: 0081, test_G: 10.20000000\n",
      "epoch: 0082, loss: 2.52952511, training_step: 2585\n",
      "epoch: 0082, test_G: 19.90000000\n",
      "epoch: 0083, loss: 2.56711938, training_step: 2711\n",
      "epoch: 0083, test_G: 15.30000000\n",
      "epoch: 0084, loss: 2.56724727, training_step: 2732\n",
      "epoch: 0084, test_G: 23.10000000\n",
      "epoch: 0085, loss: 2.54581567, training_step: 2782\n",
      "epoch: 0085, test_G: 11.30000000\n",
      "epoch: 0086, loss: 2.56501274, training_step: 2820\n",
      "epoch: 0086, test_G: 15.90000000\n",
      "epoch: 0087, loss: 2.51944436, training_step: 2876\n",
      "epoch: 0087, test_G: 13.50000000\n",
      "epoch: 0088, loss: 2.57287144, training_step: 2981\n",
      "epoch: 0088, test_G: 15.30000000\n",
      "epoch: 0089, loss: 2.60600257, training_step: 3040\n",
      "epoch: 0089, test_G: 17.90000000\n",
      "epoch: 0090, loss: 2.57534563, training_step: 3089\n",
      "epoch: 0090, test_G: 14.30000000\n",
      "epoch: 0091, loss: 2.60349484, training_step: 3146\n",
      "epoch: 0091, test_G: 10.30000000\n",
      "epoch: 0092, loss: 2.59634056, training_step: 3230\n",
      "epoch: 0092, test_G: 14.40000000\n",
      "epoch: 0093, loss: 2.55485762, training_step: 3261\n",
      "epoch: 0093, test_G: 11.50000000\n",
      "epoch: 0094, loss: 2.60660590, training_step: 3277\n",
      "epoch: 0094, test_G: 32.20000000\n",
      "epoch: 0095, loss: 2.62623998, training_step: 3410\n",
      "epoch: 0095, test_G: 26.30000000\n",
      "epoch: 0096, loss: 2.60011349, training_step: 3473\n",
      "epoch: 0096, test_G: 23.60000000\n",
      "epoch: 0097, loss: 2.62139015, training_step: 3493\n",
      "epoch: 0097, test_G: 14.10000000\n",
      "epoch: 0098, loss: 2.59052452, training_step: 3510\n",
      "epoch: 0098, test_G: 93.20000000\n",
      "epoch: 0099, loss: 2.59620238, training_step: 3544\n",
      "epoch: 0099, test_G: 10.90000000\n",
      "epoch: 0100, loss: 2.64289750, training_step: 3623\n",
      "epoch: 0100, test_G: 10.70000000\n",
      "epoch: 0101, loss: 2.64931455, training_step: 3660\n",
      "epoch: 0101, test_G: 10.80000000\n",
      "epoch: 0102, loss: 2.66564640, training_step: 3696\n",
      "epoch: 0102, test_G: 11.70000000\n",
      "epoch: 0103, loss: 2.63002303, training_step: 3715\n",
      "epoch: 0103, test_G: 17.80000000\n",
      "epoch: 0104, loss: 2.66673034, training_step: 3756\n",
      "epoch: 0104, test_G: 8.80000000\n",
      "epoch: 0105, loss: 2.70160426, training_step: 3781\n",
      "epoch: 0105, test_G: 12.40000000\n",
      "epoch: 0106, loss: 2.66281596, training_step: 3800\n",
      "epoch: 0106, test_G: 12.50000000\n",
      "epoch: 0107, loss: 2.62322471, training_step: 3833\n",
      "epoch: 0107, test_G: 10.90000000\n",
      "epoch: 0108, loss: 2.61270496, training_step: 3849\n",
      "epoch: 0108, test_G: 14.20000000\n",
      "epoch: 0109, loss: 2.68351505, training_step: 3896\n",
      "epoch: 0109, test_G: 29.30000000\n",
      "epoch: 0110, loss: 2.65982378, training_step: 3943\n",
      "epoch: 0110, test_G: 14.80000000\n",
      "epoch: 0111, loss: 2.61059788, training_step: 3958\n",
      "epoch: 0111, test_G: 16.00000000\n",
      "epoch: 0112, loss: 2.63163503, training_step: 3987\n",
      "epoch: 0112, test_G: 11.50000000\n",
      "epoch: 0113, loss: 2.67414923, training_step: 4062\n",
      "epoch: 0113, test_G: 15.90000000\n",
      "epoch: 0114, loss: 2.63822712, training_step: 4114\n",
      "epoch: 0114, test_G: 12.10000000\n",
      "epoch: 0115, loss: 2.66499949, training_step: 4143\n",
      "epoch: 0115, test_G: 17.50000000\n",
      "epoch: 0116, loss: 2.66509880, training_step: 4279\n",
      "epoch: 0116, test_G: 13.10000000\n",
      "epoch: 0117, loss: 2.68988170, training_step: 4297\n",
      "epoch: 0117, test_G: 12.70000000\n",
      "epoch: 0118, loss: 2.69652683, training_step: 4373\n",
      "epoch: 0118, test_G: 23.40000000\n",
      "epoch: 0119, loss: 2.71749409, training_step: 4416\n",
      "epoch: 0119, test_G: 18.90000000\n",
      "epoch: 0120, loss: 2.73180960, training_step: 4448\n",
      "epoch: 0120, test_G: 37.40000000\n",
      "epoch: 0121, loss: 2.66574540, training_step: 4476\n",
      "epoch: 0121, test_G: 91.00000000\n",
      "epoch: 0122, loss: 2.70945439, training_step: 4524\n",
      "epoch: 0122, test_G: 92.70000000\n",
      "epoch: 0123, loss: 2.67932131, training_step: 4558\n",
      "epoch: 0123, test_G: 38.20000000\n",
      "epoch: 0124, loss: 2.67627067, training_step: 4668\n",
      "epoch: 0124, test_G: 12.00000000\n",
      "epoch: 0125, loss: 2.71038620, training_step: 4748\n",
      "epoch: 0125, test_G: 11.80000000\n",
      "epoch: 0126, loss: 2.68625004, training_step: 4769\n",
      "epoch: 0126, test_G: 12.00000000\n",
      "epoch: 0127, loss: 2.69518519, training_step: 4869\n",
      "epoch: 0127, test_G: 24.80000000\n",
      "epoch: 0128, loss: 2.73096017, training_step: 5014\n",
      "epoch: 0128, test_G: 9.60000000\n",
      "epoch: 0129, loss: 2.71832034, training_step: 5029\n",
      "epoch: 0129, test_G: 11.50000000\n",
      "epoch: 0130, loss: 2.72217721, training_step: 5047\n",
      "epoch: 0130, test_G: 10.20000000\n",
      "epoch: 0131, loss: 2.73532224, training_step: 5157\n",
      "epoch: 0131, test_G: 26.70000000\n",
      "epoch: 0132, loss: 2.72846568, training_step: 5194\n",
      "epoch: 0132, test_G: 20.80000000\n",
      "epoch: 0133, loss: 2.73551790, training_step: 5209\n",
      "epoch: 0133, test_G: 10.70000000\n",
      "epoch: 0134, loss: 2.72300331, training_step: 5256\n",
      "epoch: 0134, test_G: 9.80000000\n",
      "epoch: 0135, loss: 2.69984942, training_step: 5275\n",
      "epoch: 0135, test_G: 10.80000000\n",
      "epoch: 0136, loss: 2.74783563, training_step: 5305\n",
      "epoch: 0136, test_G: 16.90000000\n",
      "epoch: 0137, loss: 2.77458976, training_step: 5388\n",
      "epoch: 0137, test_G: 16.60000000\n",
      "epoch: 0138, loss: 2.72644781, training_step: 5431\n",
      "epoch: 0138, test_G: 19.70000000\n",
      "epoch: 0139, loss: 2.74795136, training_step: 5454\n",
      "epoch: 0139, test_G: 11.50000000\n",
      "epoch: 0140, loss: 2.72570383, training_step: 5479\n",
      "epoch: 0140, test_G: 9.40000000\n",
      "epoch: 0141, loss: 2.72503665, training_step: 5568\n",
      "epoch: 0141, test_G: 72.80000000\n",
      "epoch: 0142, loss: 2.74806254, training_step: 5744\n",
      "epoch: 0142, test_G: 81.50000000\n",
      "epoch: 0143, loss: 2.75685377, training_step: 5812\n",
      "epoch: 0143, test_G: 88.70000000\n",
      "epoch: 0144, loss: 2.78273985, training_step: 5917\n",
      "epoch: 0144, test_G: 9.70000000\n",
      "epoch: 0145, loss: 2.73407286, training_step: 5932\n",
      "epoch: 0145, test_G: 9.30000000\n",
      "epoch: 0146, loss: 2.78255202, training_step: 5950\n",
      "epoch: 0146, test_G: 10.10000000\n",
      "epoch: 0147, loss: 2.72699333, training_step: 6002\n",
      "epoch: 0147, test_G: 14.30000000\n",
      "epoch: 0148, loss: 2.72535928, training_step: 6043\n",
      "epoch: 0148, test_G: 46.70000000\n",
      "epoch: 0149, loss: 2.77527763, training_step: 6200\n",
      "epoch: 0149, test_G: 10.30000000\n",
      "epoch: 0150, loss: 2.75165623, training_step: 6244\n",
      "epoch: 0150, test_G: 11.90000000\n",
      "epoch: 0151, loss: 2.78037278, training_step: 6263\n",
      "epoch: 0151, test_G: 12.10000000\n",
      "epoch: 0152, loss: 2.75229892, training_step: 6466\n",
      "epoch: 0152, test_G: 16.00000000\n",
      "epoch: 0153, loss: 2.73383764, training_step: 6508\n",
      "epoch: 0153, test_G: 9.70000000\n",
      "epoch: 0154, loss: 2.74255783, training_step: 6525\n",
      "epoch: 0154, test_G: 11.50000000\n",
      "epoch: 0155, loss: 2.73475803, training_step: 6586\n",
      "epoch: 0155, test_G: 10.70000000\n",
      "epoch: 0156, loss: 2.77778285, training_step: 6704\n",
      "epoch: 0156, test_G: 10.00000000\n",
      "epoch: 0157, loss: 2.78011054, training_step: 6724\n",
      "epoch: 0157, test_G: 10.00000000\n",
      "epoch: 0158, loss: 2.76220216, training_step: 6930\n",
      "epoch: 0158, test_G: 9.60000000\n",
      "epoch: 0159, loss: 2.70788006, training_step: 6941\n",
      "epoch: 0159, test_G: 17.90000000\n",
      "epoch: 0160, loss: 2.76801163, training_step: 7090\n",
      "epoch: 0160, test_G: 41.40000000\n",
      "epoch: 0161, loss: 2.75183230, training_step: 7254\n",
      "epoch: 0161, test_G: 9.50000000\n",
      "epoch: 0162, loss: 2.75112855, training_step: 7318\n",
      "epoch: 0162, test_G: 91.30000000\n",
      "epoch: 0163, loss: 2.80069679, training_step: 7359\n",
      "epoch: 0163, test_G: 38.10000000\n",
      "epoch: 0164, loss: 2.78272398, training_step: 7466\n",
      "epoch: 0164, test_G: 29.20000000\n",
      "epoch: 0165, loss: 2.77141365, training_step: 7527\n",
      "epoch: 0165, test_G: 11.80000000\n",
      "epoch: 0166, loss: 2.80920948, training_step: 7607\n",
      "epoch: 0166, test_G: 20.90000000\n",
      "epoch: 0167, loss: 2.77631760, training_step: 7721\n",
      "epoch: 0167, test_G: 16.30000000\n",
      "epoch: 0168, loss: 2.77598345, training_step: 7788\n",
      "epoch: 0168, test_G: 17.70000000\n",
      "epoch: 0169, loss: 2.78881275, training_step: 7877\n",
      "epoch: 0169, test_G: 43.50000000\n",
      "epoch: 0170, loss: 2.75754951, training_step: 7965\n",
      "epoch: 0170, test_G: 30.20000000\n",
      "epoch: 0171, loss: 2.81522938, training_step: 8062\n",
      "epoch: 0171, test_G: 119.30000000\n",
      "epoch: 0172, loss: 2.80381588, training_step: 8376\n",
      "epoch: 0172, test_G: 58.30000000\n",
      "epoch: 0173, loss: 2.79540227, training_step: 8860\n",
      "epoch: 0173, test_G: 50.10000000\n",
      "epoch: 0174, loss: 2.80420759, training_step: 9147\n",
      "epoch: 0174, test_G: 10.90000000\n",
      "epoch: 0175, loss: 2.79775287, training_step: 9646\n",
      "epoch: 0175, test_G: 51.50000000\n",
      "epoch: 0176, loss: 2.81183842, training_step: 9897\n",
      "epoch: 0176, test_G: 35.50000000\n",
      "epoch: 0177, loss: 2.78926090, training_step: 9921\n",
      "epoch: 0177, test_G: 59.10000000\n",
      "epoch: 0178, loss: 0.76078847, training_step: 10000\n",
      "epoch: 0178, test_G: 86.50000000\n",
      "Best total reward in test: 208.9\n"
     ]
    }
   ],
   "source": [
    "random_seed = 0\n",
    "discount = 0.997\n",
    "buffer_warm_up = 64\n",
    "max_training_steps = 10000\n",
    "max_episodes = 1000\n",
    "num_simulations = 50\n",
    "num_test_episodes = 10\n",
    "num_trajectory = 32\n",
    "sample_per_trajectory = 1\n",
    "k_steps = 10\n",
    "\n",
    "gradient_transform = muax.model.optimizer(init_value=0.02, peak_value=0.02, end_value=0.002, warmup_steps=5000, transition_steps=5000)\n",
    "tracer = muax.PNStep(10, discount, 0.5)\n",
    "buffer = muax.TrajectoryReplayBuffer(500)\n",
    "model = muax.MuZero(repr_fn, pred_fn, dy_fn, optimizer=gradient_transform, discount=discount)\n",
    "\n",
    "env_id = 'CartPole-v1'\n",
    "env = gym.make(env_id, render_mode='rgb_array')\n",
    "test_env = gym.make(env_id, render_mode='rgb_array')\n",
    "\n",
    "sample_input = jnp.expand_dims(jnp.zeros(env.observation_space.shape), axis=0)\n",
    "key = jax.random.PRNGKey(random_seed)\n",
    "key, test_key, subkey = jax.random.split(key, num=3)\n",
    "model.init(subkey, sample_input) \n",
    "\n",
    "training_step = 0\n",
    "best_test_G = -float('inf')\n",
    "max_training_steps_reached = False\n",
    "\n",
    "# buffer warm up\n",
    "print('buffer warm up stage...')\n",
    "while len(buffer) < buffer_warm_up:\n",
    "  obs, info = env.reset()    \n",
    "  trajectory = Trajectory()\n",
    "  temperature = temperature_fn(max_training_steps=max_training_steps, training_steps=training_step)\n",
    "  for t in range(env.spec.max_episode_steps):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    a, pi, v = model.act(subkey, obs, \n",
    "                          with_pi=True, \n",
    "                          with_value=True, \n",
    "                          obs_from_batch=False,\n",
    "                          num_simulations=num_simulations,\n",
    "                          temperature=temperature)\n",
    "    obs_next, r, done, truncated, info = env.step(a)\n",
    "    if truncated:\n",
    "      r = 1 / (1 - tracer.gamma)\n",
    "    tracer.add(obs, a, r, done or truncated, v=v, pi=pi)\n",
    "    while tracer:\n",
    "      trans = tracer.pop()\n",
    "      trajectory.add(trans)\n",
    "    if done or truncated:\n",
    "      break \n",
    "    obs = obs_next \n",
    "  trajectory.finalize()\n",
    "  if len(trajectory) >= k_steps:\n",
    "    buffer.add(trajectory, trajectory.batched_transitions.w.mean())\n",
    "  \n",
    "print('start training...')\n",
    "  \n",
    "for ep in range(max_episodes):\n",
    "  obs, info = env.reset(seed=random_seed)    \n",
    "  trajectory = Trajectory()\n",
    "  temperature = temperature_fn(max_training_steps=max_training_steps, training_steps=training_step)\n",
    "  for t in range(env.spec.max_episode_steps):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    a, pi, v = model.act(subkey, obs, \n",
    "                          with_pi=True, \n",
    "                          with_value=True, \n",
    "                          obs_from_batch=False,\n",
    "                          num_simulations=num_simulations,\n",
    "                          temperature=temperature)\n",
    "    obs_next, r, done, truncated, info = env.step(a)\n",
    "    if truncated:\n",
    "      r = 1 / (1 - tracer.gamma)\n",
    "    tracer.add(obs, a, r, done or truncated, v=v, pi=pi)\n",
    "    while tracer:\n",
    "      trans = tracer.pop()\n",
    "      trajectory.add(trans)\n",
    "    if done or truncated:\n",
    "      break \n",
    "    obs = obs_next \n",
    "  trajectory.finalize()\n",
    "  if len(trajectory) >= k_steps:\n",
    "    buffer.add(trajectory, trajectory.batched_transitions.w.mean())\n",
    "  \n",
    "  if max_training_steps_reached:\n",
    "    break\n",
    "  train_loss = 0\n",
    "  for _ in range(t):\n",
    "    transition_batch = buffer.sample(num_trajectory=num_trajectory,\n",
    "                                      sample_per_trajectory=sample_per_trajectory,\n",
    "                                      k_steps=k_steps)\n",
    "    loss_metric = model.update(transition_batch)\n",
    "    train_loss += loss_metric['loss']\n",
    "    training_step += 1\n",
    "    if training_step >= max_training_steps:\n",
    "      max_training_steps_reached = True\n",
    "      break\n",
    "  train_loss /= t\n",
    "  print(f'epoch: {ep:04d}, loss: {train_loss:.8f}, training_step: {training_step}')\n",
    "\n",
    "  test_G = test(model, test_env, test_key, num_simulations=num_simulations, num_test_episodes=num_test_episodes)      \n",
    "  print(f'epoch: {ep:04d}, test_G: {test_G:.8f}')\n",
    "  if test_G >= best_test_G:\n",
    "    best_test_G = test_G\n",
    "\n",
    "print(f'Best total reward in test: {best_test_G}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
